{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ef94728",
      "metadata": {},
      "source": [
        "# SDXL Model Pipeline Setup - Lightning Fix Applied\n",
        "Supports 9 distillation models with proper scheduler configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02129d5a",
      "metadata": {
        "gather": {
          "logged": 1761949894455
        }
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Type\n",
        "import torch\n",
        "from diffusers import (\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionXLPipeline,\n",
        "    UNet2DConditionModel,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    LCMScheduler,\n",
        "    TCDScheduler,\n",
        ")\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "SUPPORTED_DISTILLATIONS = [\n",
        "    \"base\", \"dmd\", \"lightning\", \"turbo\", \"lcm\", \"hyper\", \"pcm\", \"tcd\", \"flash\"\n",
        "]\n",
        "\n",
        "def load_pipeline(\n",
        "    pipeline_cls: Type[DiffusionPipeline],\n",
        "    distillation_type: Optional[str] = \"base\",\n",
        "    *,\n",
        "    model_id: str = \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    \n",
        "    weights_dtype = torch.bfloat16,  # match your manual snippet\n",
        ") -> DiffusionPipeline:\n",
        "    \"\"\"\n",
        "    Build the requested SDXL pipeline class in the same way as the manual code:\n",
        "      1) Borrow scheduler config from a plain SDXL base pipeline (not the target pipeline_cls).\n",
        "      2) Load/prepare distilled UNet (and move it to device/dtype immediately).\n",
        "      3) Instantiate the requested pipeline with variant='fp16' and the (possibly) overridden UNet.\n",
        "      4) Attach the method-appropriate scheduler using the borrowed base config.\n",
        "      5) Move the final pipeline to `device` (no extra dtype cast here).\n",
        "\n",
        "    Notes:\n",
        "      - SDXL-Turbo/Lightning expect Euler(A)/Euler with trailing timesteps; LCM/Flash use LCMScheduler; PCM uses DDIM; TCD uses TCDScheduler.\n",
        "      - CFG/steps expectations are method-specific and should be set at sampling time.\n",
        "    \"\"\"\n",
        "    kind = \"base\" if distillation_type in (None, \"base\") else str(distillation_type).lower()\n",
        "    if kind not in SUPPORTED_DISTILLATIONS:\n",
        "        raise ValueError(\n",
        "            f\"Unknown distillation type: '{distillation_type}'. \"\n",
        "            f\"Available: {', '.join(SUPPORTED_DISTILLATIONS)}\"\n",
        "        )\n",
        "\n",
        "    # --- 1) Borrow scheduler config from a plain SDXL base (matches your manual code) ---\n",
        "    base_cfg_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=weights_dtype,\n",
        "        use_safetensors=True,\n",
        "    )\n",
        "    base_sched_config = base_cfg_pipe.scheduler.config\n",
        "    unet_config = base_cfg_pipe.unet.config  # handy for from_config() cases\n",
        "    del base_cfg_pipe  # keep VRAM tidy\n",
        "\n",
        "    # --- 2) Prepare distilled UNet / adapters (move UNet to device+dtype immediately) ---\n",
        "    unet_override = None\n",
        "\n",
        "    if kind == \"base\":\n",
        "        pass\n",
        "\n",
        "    elif kind == \"dmd\":\n",
        "        distilled_unet = UNet2DConditionModel.from_config(unet_config)\n",
        "        repo, fname = \"tianweiy/DMD2\", \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
        "        state = torch.load(hf_hub_download(repo, fname), map_location=\"cpu\")\n",
        "        if isinstance(state, dict) and \"state_dict\" in state:\n",
        "            state = state[\"state_dict\"]\n",
        "        distilled_unet.load_state_dict(state, strict=True)\n",
        "        distilled_unet = distilled_unet.to(device, dtype=weights_dtype)\n",
        "        unet_override = distilled_unet\n",
        "\n",
        "    elif kind == \"lightning\":\n",
        "        distilled_unet = UNet2DConditionModel.from_config(unet_config)\n",
        "        repo, fname = \"ByteDance/SDXL-Lightning\", \"sdxl_lightning_4step_unet.safetensors\"\n",
        "        state = load_file(hf_hub_download(repo, fname))\n",
        "        distilled_unet.load_state_dict(state, strict=True)\n",
        "        distilled_unet = distilled_unet.to(device, dtype=weights_dtype)\n",
        "        unet_override = distilled_unet\n",
        "\n",
        "    elif kind == \"turbo\":\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
        "            \"stabilityai/sdxl-turbo\",\n",
        "            subfolder=\"unet\",\n",
        "            torch_dtype=weights_dtype,\n",
        "            variant=\"fp16\",\n",
        "        ).to(device, dtype=weights_dtype)\n",
        "        unet_override = distilled_unet\n",
        "\n",
        "    elif kind == \"lcm\":\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
        "            \"latent-consistency/lcm-sdxl\",\n",
        "            torch_dtype=weights_dtype,\n",
        "        ).to(device, dtype=weights_dtype)\n",
        "        unet_override = distilled_unet\n",
        "\n",
        "    # LoRA-style adapters are applied after the main pipeline is built (no unet_override).\n",
        "    # hyper, pcm, tcd, flash handled below.\n",
        "\n",
        "    # --- 3) Build the caller’s pipeline with variant='fp16' and optional UNet override ---\n",
        "    if unet_override is not None:\n",
        "        pipe = pipeline_cls.from_pretrained(\n",
        "            model_id,\n",
        "            unet=unet_override,\n",
        "            torch_dtype=weights_dtype,\n",
        "            variant=\"fp16\",\n",
        "            use_safetensors=True,\n",
        "        )\n",
        "    else:\n",
        "        pipe = pipeline_cls.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=weights_dtype,\n",
        "            variant=\"fp16\",\n",
        "            use_safetensors=True,\n",
        "        )\n",
        "\n",
        "    # Apply LoRA adapters where relevant (kept consistent with the previous implementation)\n",
        "    if kind == \"hyper\":\n",
        "        pipe.load_lora_weights(\n",
        "            \"ByteDance/Hyper-SD\",\n",
        "            weight_name=\"Hyper-SDXL-8steps-CFG-lora.safetensors\",\n",
        "            adapter_name=\"hyper-sdxl-8step\",\n",
        "        )\n",
        "        pipe.fuse_lora()\n",
        "\n",
        "    elif kind == \"pcm\":\n",
        "        pipe.load_lora_weights(\n",
        "            \"wangfuyun/PCM_Weights\",\n",
        "            weight_name=\"pcm_sdxl_smallcfg_4step_converted.safetensors\",\n",
        "            subfolder=\"sdxl\",\n",
        "            adapter_name=\"pcm-lora\",\n",
        "        )\n",
        "        pipe.fuse_lora()\n",
        "\n",
        "    elif kind == \"tcd\":\n",
        "        pipe.load_lora_weights(\"h1t/TCD-SDXL-LoRA\", adapter_name=\"tcd-lora\")\n",
        "        pipe.set_adapters([\"tcd-lora\"], adapter_weights=[1.0])\n",
        "\n",
        "    elif kind == \"flash\":\n",
        "        pipe.load_lora_weights(\n",
        "            \"jasperai/flash-sdxl\",\n",
        "            weight_name=\"pytorch_lora_weights.safetensors\",\n",
        "            adapter_name=\"flash-sdxl\",\n",
        "        )\n",
        "        pipe.set_adapters([\"flash-sdxl\"], adapter_weights=[1.0])\n",
        "        pipe.fuse_lora()\n",
        "\n",
        "    # --- 4) Attach the method-appropriate scheduler (EulerA for Turbo; trailing timesteps) ---\n",
        "    if kind in (\"base\",):\n",
        "        # keep the default scheduler already on the pipeline\n",
        "        pass\n",
        "    elif kind in (\"lcm\", \"dmd\", \"flash\"):\n",
        "        pipe.scheduler = LCMScheduler.from_config(base_sched_config)\n",
        "    elif kind == \"lightning\":\n",
        "        pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
        "            base_sched_config, timestep_spacing=\"trailing\"\n",
        "        )\n",
        "    elif kind == \"turbo\":\n",
        "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "            base_sched_config, timestep_spacing=\"trailing\"\n",
        "        )\n",
        "    elif kind == \"pcm\":\n",
        "        pipe.scheduler = DDIMScheduler.from_config(\n",
        "            base_sched_config,\n",
        "            timestep_spacing=\"trailing\",\n",
        "            clip_sample=False,\n",
        "            set_alpha_to_one=False,\n",
        "        )\n",
        "    elif kind == \"tcd\":\n",
        "        pipe.scheduler = TCDScheduler.from_config(base_sched_config)\n",
        "    elif kind == \"hyper\":\n",
        "        pipe.scheduler = DDIMScheduler.from_config(\n",
        "            base_sched_config, timestep_spacing=\"trailing\"\n",
        "        )\n",
        "\n",
        "    # --- 5) Move whole pipeline to device (no dtype re-cast here; matches manual code) ---\n",
        "    pipe.to(device)\n",
        "    return pipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c58c56",
      "metadata": {
        "gather": {
          "logged": 1761949894525
        }
      },
      "outputs": [],
      "source": [
        "# NAG Attention Processor - from attention_nag.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from diffusers.utils import deprecate\n",
        "from diffusers.models.attention_processor import Attention\n",
        "\n",
        "class NAGAttnProcessor2_0:\n",
        "    \"\"\"\n",
        "    Normalized Attention Guidance (NAG) attention processor using PyTorch 2.0's\n",
        "    scaled_dot_product_attention for efficient computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nag_scale: float = 1.0, nag_tau: float = 2.5, nag_alpha: float = 0.5):\n",
        "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
        "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
        "        self.nag_scale = nag_scale\n",
        "        self.nag_tau = nag_tau\n",
        "        self.nag_alpha = nag_alpha\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        attn,\n",
        "        hidden_states: torch.Tensor,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        temb: Optional[torch.Tensor] = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
        "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
        "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
        "\n",
        "        residual = hidden_states\n",
        "        if attn.spatial_norm is not None:\n",
        "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
        "\n",
        "        input_ndim = hidden_states.ndim\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            batch_size, channel, height, width = hidden_states.shape\n",
        "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
        "\n",
        "        batch_size, sequence_length, _ = (\n",
        "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
        "        )\n",
        "\n",
        "        apply_guidance = self.nag_scale > 1 and encoder_hidden_states is not None\n",
        "        if apply_guidance:\n",
        "            origin_batch_size = batch_size - len(hidden_states)\n",
        "            assert batch_size / origin_batch_size in [2, 3, 4]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
        "\n",
        "        if attn.group_norm is not None:\n",
        "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = attn.to_q(hidden_states)\n",
        "\n",
        "        if encoder_hidden_states is None:\n",
        "            encoder_hidden_states = hidden_states\n",
        "        elif attn.norm_cross:\n",
        "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
        "\n",
        "        key = attn.to_k(encoder_hidden_states)\n",
        "        value = attn.to_v(encoder_hidden_states)\n",
        "\n",
        "        inner_dim = key.shape[-1]\n",
        "        head_dim = inner_dim // attn.heads\n",
        "\n",
        "        if apply_guidance:\n",
        "            if batch_size == 2 * origin_batch_size:\n",
        "                query = query.tile(2, 1, 1)\n",
        "            else:\n",
        "                query = torch.cat((query, query[origin_batch_size:2 * origin_batch_size]), dim=0)\n",
        "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        if attn.norm_q is not None:\n",
        "            query = attn.norm_q(query)\n",
        "        if attn.norm_k is not None:\n",
        "            key = attn.norm_k(key)\n",
        "\n",
        "        hidden_states = F.scaled_dot_product_attention(\n",
        "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "        )\n",
        "\n",
        "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
        "        hidden_states = hidden_states.to(query.dtype)\n",
        "\n",
        "        if apply_guidance:\n",
        "            hidden_states_negative = hidden_states[-origin_batch_size:]\n",
        "            if batch_size == 2 * origin_batch_size:\n",
        "                hidden_states_positive = hidden_states[:origin_batch_size]\n",
        "            else:\n",
        "                hidden_states_positive = hidden_states[origin_batch_size:2 * origin_batch_size]\n",
        "            hidden_states_guidance = hidden_states_positive * self.nag_scale - hidden_states_negative * (self.nag_scale - 1)\n",
        "            norm_positive = torch.norm(hidden_states_positive, p=1, dim=-1, keepdim=True).expand(*hidden_states_positive.shape)\n",
        "            norm_guidance = torch.norm(hidden_states_guidance, p=1, dim=-1, keepdim=True).expand(*hidden_states_guidance.shape)\n",
        "\n",
        "            scale = norm_guidance / norm_positive\n",
        "            hidden_states_guidance = hidden_states_guidance * torch.minimum(scale, scale.new_ones(1) * self.nag_tau) / scale\n",
        "\n",
        "            hidden_states_guidance = hidden_states_guidance * self.nag_alpha + hidden_states_positive * (1 - self.nag_alpha)\n",
        "\n",
        "            if batch_size == 2 * origin_batch_size:\n",
        "                hidden_states = hidden_states_guidance\n",
        "            elif batch_size == 3 * origin_batch_size:\n",
        "                hidden_states = torch.cat((hidden_states[:origin_batch_size], hidden_states_guidance), dim=0)\n",
        "            elif batch_size == 4 * origin_batch_size:\n",
        "                hidden_states = torch.cat((hidden_states[:origin_batch_size], hidden_states_guidance, hidden_states[2 * origin_batch_size:3 * origin_batch_size]), dim=0)\n",
        "\n",
        "        hidden_states = attn.to_out[0](hidden_states)\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
        "\n",
        "        if attn.residual_connection:\n",
        "            hidden_states = hidden_states + residual\n",
        "\n",
        "        hidden_states = hidden_states / attn.rescale_output_factor\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "print(\"✓ NAGAttnProcessor2_0 loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c35480c",
      "metadata": {
        "gather": {
          "logged": 1761949895656
        }
      },
      "outputs": [],
      "source": [
        "# NAG Pipeline with Timing - from pipeline_sdxl_impactful_nag.py\n",
        "import math\n",
        "from diffusers.callbacks import MultiPipelineCallbacks, PipelineCallback\n",
        "from diffusers.image_processor import PipelineImageInput\n",
        "from diffusers.utils import deprecate, is_torch_xla_available\n",
        "from diffusers.pipelines.stable_diffusion_xl.pipeline_output import StableDiffusionXLPipelineOutput\n",
        "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import (\n",
        "    retrieve_timesteps,\n",
        "    rescale_noise_cfg,\n",
        ")\n",
        "from typing import Union, List, Optional, Dict, Any, Tuple, Callable\n",
        "\n",
        "if is_torch_xla_available():\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    XLA_AVAILABLE = True\n",
        "else:\n",
        "    XLA_AVAILABLE = False\n",
        "\n",
        "# Now the timing-aware variant\n",
        "class NAGTimeStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n",
        "\n",
        "    \"\"\"Base NAG pipeline that extends StableDiffusionXLPipeline\"\"\"\n",
        "    \n",
        "    @property\n",
        "    def do_normalized_attention_guidance(self):\n",
        "        return self._nag_scale > 1\n",
        "\n",
        "    def _set_nag_attn_processor(self, nag_scale, nag_tau=2.5, nag_alpha=0.5):\n",
        "        if self.do_normalized_attention_guidance:\n",
        "            attn_procs = {}\n",
        "            for name, origin_attn_processor in self.unet.attn_processors.items():\n",
        "                if \"attn2\" in name:\n",
        "                    attn_procs[name] = NAGAttnProcessor2_0(nag_scale=nag_scale, nag_tau=nag_tau, nag_alpha=nag_alpha)\n",
        "                else:\n",
        "                    attn_procs[name] = origin_attn_processor\n",
        "            self.unet.set_attn_processor(attn_procs)\n",
        "\n",
        "    \"\"\"\n",
        "    NAG with timing from 'Impact of Negative Prompts':\n",
        "    - Delay enabling NAG until nag_start fraction of the trajectory\n",
        "    - Optional linear ramp of nag_scale for nag_ramp_steps after start\n",
        "    - Keep existing nag_end behaviour\n",
        "\n",
        "    Backwards-compatible cool-down (time-based):\n",
        "    - Optional post-NAG cool-down that temporarily reduces CFG by a fraction.\n",
        "    - Disabled by default (nag_cooldown=0.0) to preserve existing behaviour.\n",
        "    \"\"\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        timesteps: List[int] = None,\n",
        "        sigmas: List[float] = None,\n",
        "        denoising_end: Optional[float] = None,\n",
        "        guidance_scale: float = 5.0,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: Optional[int] = 1,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.Tensor] = None,\n",
        "        prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
        "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        guidance_rescale: float = 0.0,\n",
        "        original_size: Optional[Tuple[int, int]] = None,\n",
        "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        target_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
        "        clip_skip: Optional[int] = None,\n",
        "        callback_on_step_end: Optional[\n",
        "            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
        "        ] = None,\n",
        "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
        "\n",
        "        # NAG controls\n",
        "        nag_scale: float = 2.0,\n",
        "        nag_tau: float = 2.5,\n",
        "        nag_alpha: float = 0.5,\n",
        "        nag_negative_prompt: str = None,\n",
        "        nag_negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        nag_end: float = 1.0,\n",
        "\n",
        "        # Timing knobs from Impact of Negative Prompts\n",
        "        nag_start: float = 0.2,          # start NAG after 20% of the trajectory\n",
        "        nag_ramp_steps: int = 0,         # optional soft-start; 0 disables ramp\n",
        "\n",
        "        # NEW: time-based cool-down (fully backward compatible when 0.0)\n",
        "        nag_cooldown: float = 0.0,          # fraction after nag_end to apply cool-down (0.0 = disabled)\n",
        "        nag_cooldown_cfg_drop: float = 0.2, # relative CFG reduction during cool-down (e.g., 0.2 = -20%)\n",
        "\n",
        "        **kwargs,\n",
        "    ):\n",
        "        callback = kwargs.pop(\"callback\", None)\n",
        "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
        "\n",
        "        if callback is not None:\n",
        "            deprecate(\n",
        "                \"callback\",\n",
        "                \"1.0.0\",\n",
        "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
        "            )\n",
        "        if callback_steps is not None:\n",
        "            deprecate(\n",
        "                \"callback_steps\",\n",
        "                \"1.0.0\",\n",
        "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
        "            )\n",
        "\n",
        "        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
        "            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
        "\n",
        "        # 0. Defaults\n",
        "        height = height or self.default_sample_size * self.vae_scale_factor\n",
        "        width = width or self.default_sample_size * self.vae_scale_factor\n",
        "        original_size = original_size or (height, width)\n",
        "        target_size = target_size or (height, width)\n",
        "\n",
        "        # 1. Check inputs\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            prompt_2,\n",
        "            height,\n",
        "            width,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            negative_prompt_2,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "            ip_adapter_image,\n",
        "            ip_adapter_image_embeds,\n",
        "            callback_on_step_end_tensor_inputs,\n",
        "        )\n",
        "\n",
        "        self._guidance_scale = guidance_scale\n",
        "        self._guidance_rescale = guidance_rescale\n",
        "        self._clip_skip = clip_skip\n",
        "        self._cross_attention_kwargs = cross_attention_kwargs\n",
        "        self._denoising_end = denoising_end\n",
        "        self._interrupt = False\n",
        "        self._nag_scale = nag_scale\n",
        "\n",
        "        # 2. Batch size\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        # 3. Encode prompts\n",
        "        lora_scale = self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
        "\n",
        "        (\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        ) = self.encode_prompt(\n",
        "            prompt=prompt,\n",
        "            prompt_2=prompt_2,\n",
        "            device=device,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            do_classifier_free_guidance=self.do_classifier_free_guidance or self.do_normalized_attention_guidance,\n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_2=negative_prompt_2,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
        "            lora_scale=lora_scale,\n",
        "            clip_skip=self.clip_skip,\n",
        "        )\n",
        "\n",
        "        # Prepare NAG negative embeddings (but DO NOT append yet; we enable later)\n",
        "        if self.do_normalized_attention_guidance:\n",
        "            if nag_negative_prompt_embeds is None:\n",
        "                if nag_negative_prompt is None:\n",
        "                    if negative_prompt is not None:\n",
        "                        if self.do_classifier_free_guidance:\n",
        "                            nag_negative_prompt_embeds = negative_prompt_embeds\n",
        "                        else:\n",
        "                            nag_negative_prompt = negative_prompt\n",
        "                    else:\n",
        "                        nag_negative_prompt = \"\"\n",
        "                if nag_negative_prompt is not None and nag_negative_prompt_embeds is None:\n",
        "                    nag_negative_prompt_embeds = self.encode_prompt(\n",
        "                        prompt=nag_negative_prompt,\n",
        "                        device=device,\n",
        "                        num_images_per_prompt=num_images_per_prompt,\n",
        "                        do_classifier_free_guidance=False,\n",
        "                        lora_scale=lora_scale,\n",
        "                        clip_skip=self.clip_skip,\n",
        "                    )[0]\n",
        "\n",
        "        # 4. Timesteps\n",
        "        timesteps, num_inference_steps = retrieve_timesteps(\n",
        "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
        "        )\n",
        "\n",
        "        # 5. Latents\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_images_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            width,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "\n",
        "        # 6. Extra step kwargs\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # 7. Added time ids & embeddings\n",
        "        add_text_embeds = pooled_prompt_embeds\n",
        "        if self.text_encoder_2 is None:\n",
        "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
        "        else:\n",
        "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
        "\n",
        "        add_time_ids = self._get_add_time_ids(\n",
        "            original_size,\n",
        "            crops_coords_top_left,\n",
        "            target_size,\n",
        "            dtype=prompt_embeds.dtype,\n",
        "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
        "        )\n",
        "        if negative_original_size is not None and negative_target_size is not None:\n",
        "            negative_add_time_ids = self._get_add_time_ids(\n",
        "                negative_original_size,\n",
        "                negative_crops_coords_top_left,\n",
        "                negative_target_size,\n",
        "                dtype=prompt_embeds.dtype,\n",
        "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
        "            )\n",
        "        else:\n",
        "            negative_add_time_ids = add_time_ids\n",
        "\n",
        "        if self.do_classifier_free_guidance:\n",
        "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
        "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
        "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
        "\n",
        "        # NOTE: Delayed NAG — do NOT append nag_negative_prompt_embeds yet.\n",
        "\n",
        "        prompt_embeds = prompt_embeds.to(device)\n",
        "        add_text_embeds = add_text_embeds.to(device)\n",
        "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
        "\n",
        "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
        "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
        "                ip_adapter_image,\n",
        "                ip_adapter_image_embeds,\n",
        "                device,\n",
        "                batch_size * num_images_per_prompt,\n",
        "                self.do_classifier_free_guidance,\n",
        "            )\n",
        "\n",
        "        # 8. Denoising setup\n",
        "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
        "\n",
        "        # 8.1 denoising_end\n",
        "        if (\n",
        "            self.denoising_end is not None\n",
        "            and isinstance(self.denoising_end, float)\n",
        "            and 0 < self.denoising_end < 1\n",
        "        ):\n",
        "            discrete_timestep_cutoff = int(\n",
        "                round(\n",
        "                    self.scheduler.config.num_train_timesteps\n",
        "                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
        "                )\n",
        "            )\n",
        "            num_inference_steps = len([ts for ts in timesteps if ts >= discrete_timestep_cutoff])\n",
        "            timesteps = timesteps[:num_inference_steps]\n",
        "\n",
        "        # 9. Guidance scale embedding (base / default)\n",
        "        # For full backward-compatibility, we precompute the \"base\" embedding as before.\n",
        "        base_timestep_cond = None\n",
        "        if self.unet.config.time_cond_proj_dim is not None:\n",
        "            base_gs_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
        "            base_timestep_cond = self.get_guidance_scale_embedding(\n",
        "                base_gs_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
        "            ).to(device=device, dtype=latents.dtype)\n",
        "\n",
        "        # ---- NEW: timing state for NAG & cool-down ----\n",
        "        def _to_ddpm(frac: float) -> int:\n",
        "            # clamp to [0,1], map to DDPM 0..999, note timesteps are descending\n",
        "            frac = max(0.0, min(1.0, float(frac)))\n",
        "            return math.floor((1 - frac) * 999)\n",
        "\n",
        "        nag_start_t = _to_ddpm(nag_start)\n",
        "        nag_end_t = _to_ddpm(nag_end)\n",
        "\n",
        "        # Cool-down window [nag_end, nag_end + nag_cooldown] in fractional time, then map to DDPM t.\n",
        "        cooldown_active = float(nag_cooldown) > 0.0\n",
        "        if cooldown_active:\n",
        "            cooldown_end_frac = min(1.0, float(nag_end) + float(nag_cooldown))\n",
        "            cooldown_end_t = _to_ddpm(cooldown_end_frac)\n",
        "        else:\n",
        "            cooldown_end_t = None  # unused\n",
        "\n",
        "        origin_attn_procs = self.unet.attn_processors\n",
        "        attn_procs_applied = False\n",
        "        attn_procs_recovered = False\n",
        "        i_start = None  # loop index when NAG is enabled\n",
        "\n",
        "        self._num_timesteps = len(timesteps)\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                if self.interrupt:\n",
        "                    continue\n",
        "\n",
        "                # Build latent input\n",
        "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # ---- Enable NAG once we reach nag_start ----\n",
        "                if (\n",
        "                    self.do_normalized_attention_guidance\n",
        "                    and not attn_procs_applied\n",
        "                    and t <= nag_start_t\n",
        "                ):\n",
        "                    # initial scale for ramp\n",
        "                    current_scale = 1.0 if nag_ramp_steps > 0 else nag_scale\n",
        "                    self._set_nag_attn_processor(current_scale, nag_tau, nag_alpha)\n",
        "\n",
        "                    # append the NAG negative branch now\n",
        "                    if nag_negative_prompt_embeds is not None:\n",
        "                        prompt_embeds = torch.cat([prompt_embeds, nag_negative_prompt_embeds], dim=0)\n",
        "\n",
        "                    attn_procs_applied = True\n",
        "                    i_start = i  # remember start index\n",
        "\n",
        "                # ---- Optionally ramp the nag_scale for a few steps after start ----\n",
        "                if (\n",
        "                    self.do_normalized_attention_guidance\n",
        "                    and attn_procs_applied\n",
        "                    and not attn_procs_recovered\n",
        "                    and nag_ramp_steps > 0\n",
        "                    and i_start is not None\n",
        "                ):\n",
        "                    steps_since_on = max(0, i - i_start)\n",
        "                    if steps_since_on <= nag_ramp_steps:\n",
        "                        ramped = 1.0 + (nag_scale - 1.0) * (steps_since_on / max(1, nag_ramp_steps))\n",
        "                        self._set_nag_attn_processor(ramped, nag_tau, nag_alpha)\n",
        "                    else:\n",
        "                        # ensure final scale is set once ramp completes\n",
        "                        if abs(self._nag_scale - nag_scale) > 1e-6:\n",
        "                            self._set_nag_attn_processor(nag_scale, nag_tau, nag_alpha)\n",
        "\n",
        "                # ---- Disable NAG when we pass nag_end ----\n",
        "                if (\n",
        "                    self.do_normalized_attention_guidance\n",
        "                    and attn_procs_applied\n",
        "                    and not attn_procs_recovered\n",
        "                    and t < nag_end_t\n",
        "                ):\n",
        "                    self.unet.set_attn_processor(origin_attn_procs)\n",
        "                    # drop the appended branch so shapes match again\n",
        "                    prompt_embeds = prompt_embeds[: len(latent_model_input)]\n",
        "                    attn_procs_recovered = True\n",
        "\n",
        "                # ---- Cool-down (time-based) effective CFG ----\n",
        "                if cooldown_active:\n",
        "                    # In DDPM indexing, smaller t means later in the trajectory.\n",
        "                    # We are \"in cool-down\" if we've gone *past* nag_end (t <= nag_end_t)\n",
        "                    # but not yet beyond the cool-down window end (t >= cooldown_end_t).\n",
        "                    in_cooldown = (t <= nag_end_t) and (t >= cooldown_end_t)\n",
        "                else:\n",
        "                    in_cooldown = False\n",
        "\n",
        "                if in_cooldown:\n",
        "                    current_guidance_scale = max(1.0, self.guidance_scale * (1.0 - float(nag_cooldown_cfg_drop)))\n",
        "                else:\n",
        "                    current_guidance_scale = self.guidance_scale\n",
        "\n",
        "                # Build timestep_cond:\n",
        "                # - If no cool-down (default), reuse base embedding (backwards-compatible).\n",
        "                # - If cool-down is active, rebuild embedding with the step's effective CFG.\n",
        "                if self.unet.config.time_cond_proj_dim is not None:\n",
        "                    if cooldown_active:\n",
        "                        gs_tensor = torch.tensor(current_guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
        "                        timestep_cond = self.get_guidance_scale_embedding(\n",
        "                            gs_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
        "                        ).to(device=device, dtype=latents.dtype)\n",
        "                    else:\n",
        "                        timestep_cond = base_timestep_cond\n",
        "                else:\n",
        "                    timestep_cond = None\n",
        "\n",
        "                # predict noise\n",
        "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
        "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
        "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
        "\n",
        "                noise_pred = self.unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    timestep_cond=timestep_cond,\n",
        "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
        "                    added_cond_kwargs=added_cond_kwargs,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "\n",
        "                # CFG (use effective guidance for this step; identical to old behaviour when cool-down is off)\n",
        "                if self.do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + current_guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
        "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
        "\n",
        "                # step scheduler\n",
        "                latents_dtype = latents.dtype\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
        "                if latents.dtype != latents_dtype and torch.backends.mps.is_available():\n",
        "                    latents = latents.to(latents_dtype)\n",
        "\n",
        "                # callbacks\n",
        "                if callback_on_step_end is not None:\n",
        "                    callback_kwargs = {}\n",
        "                    for k in callback_on_step_end_tensor_inputs:\n",
        "                        callback_kwargs[k] = locals()[k]\n",
        "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
        "\n",
        "                    latents = callback_outputs.pop(\"latents\", latents)\n",
        "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
        "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
        "                    add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
        "                    negative_pooled_prompt_embeds = callback_outputs.pop(\n",
        "                        \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n",
        "                    )\n",
        "                    add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
        "\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
        "                        callback(step_idx, t, latents)\n",
        "\n",
        "                if XLA_AVAILABLE:\n",
        "                    xm.mark_step()\n",
        "\n",
        "        # decode\n",
        "        if output_type != \"latent\":\n",
        "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
        "            if needs_upcasting:\n",
        "                self.upcast_vae()\n",
        "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
        "            elif latents.dtype != self.vae.dtype and torch.backends.mps.is_available():\n",
        "                self.vae = self.vae.to(latents.dtype)\n",
        "\n",
        "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
        "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
        "            if has_latents_mean and has_latents_std:\n",
        "                latents_mean = torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
        "                latents_std = torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
        "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
        "            else:\n",
        "                latents = latents / self.vae.config.scaling_factor\n",
        "\n",
        "            image = self.vae.decode(latents, return_dict=False)[0]\n",
        "\n",
        "            if needs_upcasting:\n",
        "                self.vae.to(dtype=torch.float16)\n",
        "        else:\n",
        "            image = latents\n",
        "\n",
        "        if output_type != \"latent\":\n",
        "            if self.watermark is not None:\n",
        "                image = self.watermark.apply_watermark(image)\n",
        "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
        "\n",
        "        # ensure processors are restored\n",
        "        if self.do_normalized_attention_guidance and not attn_procs_recovered:\n",
        "            self.unet.set_attn_processor(origin_attn_procs)\n",
        "\n",
        "        self.maybe_free_model_hooks()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image,)\n",
        "        return StableDiffusionXLPipelineOutput(images=image)\n",
        "\n",
        "print(\"✓ NAGTimeStableDiffusionXLPipeline loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b0adb1",
      "metadata": {},
      "source": [
        "## Test Across Select Models\n",
        "Run tests on multiple models with first 10 prompts and first seed, organized by model folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1774ad30",
      "metadata": {
        "gather": {
          "logged": 1761895983935
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display as notebook_display\n",
        "\n",
        "\n",
        "# ---------- Toggle dry-run mode ----------\n",
        "dry_run = True  # When True: process only 1 prompt and display images instead of saving\n",
        "\n",
        "# ---------- Load prompts from JSON file ----------\n",
        "#prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_noun_negative.json\"\n",
        "prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_general-2.json\"\n",
        "\n",
        "model_configs = {\n",
        "#    'base': {'steps': 30, 'recommended_cfg': 7.0},\n",
        "    'dmd': {'steps': 4, 'recommended_cfg': 1.5},\n",
        "    'turbo': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lcm': {'steps': 4, 'recommended_cfg': 1.5},\n",
        "    'hyper': {'steps': 8, 'recommended_cfg': 5.0},\n",
        "    'pcm': {'steps': 4, 'recommended_cfg': 1.0},\n",
        "}\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "\n",
        "def clear_cuda(*objs):\n",
        "    \"\"\"Free refs, collect Python garbage, then flush CUDA caches.\"\"\"\n",
        "    for o in objs:\n",
        "        try:\n",
        "            del o\n",
        "        except NameError:\n",
        "            pass\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "clear_cuda()\n",
        "\n",
        "use_nag = True  # Set to True to use NAG pipeline, False for standard SDXL pipeline\n",
        "\n",
        "# ---- Fixed seeds for reproducibility (3 seeds) ----\n",
        "fixed_seeds = [42]\n",
        "\n",
        "# -------- Helpers --------\n",
        "def write_or_display(image, filepath, title=\"preview\"):\n",
        "    if dry_run:\n",
        "        # Always display inline in the notebook\n",
        "        notebook_display(image)\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        image.save(filepath)\n",
        "\n",
        "\n",
        "# Load prompts\n",
        "with open(prompts_file, 'r') as f:\n",
        "    prompts_data = json.load(f)\n",
        "\n",
        "# ---- Select top 50 prompts (or 1 in dry-run) ----\n",
        "if isinstance(prompts_data, list) and len(prompts_data) > 0 and isinstance(prompts_data[0], dict) and 'score' in prompts_data[0]:\n",
        "    prompts_data = sorted(prompts_data, key=lambda x: x.get('score', 0), reverse=True)[:50]\n",
        "else:\n",
        "    prompts_data = prompts_data[:50]\n",
        "\n",
        "if dry_run:\n",
        "    prompts_data = prompts_data[:1]  # only one prompt\n",
        "\n",
        "# ---- Choose seeds (all vs one when dry-run) ----\n",
        "seeds_to_use = fixed_seeds[:1] if dry_run else fixed_seeds\n",
        "\n",
        "print(f\"Loaded {len(prompts_data)} prompt(s) from {prompts_file}\")\n",
        "print(f\"[CONFIG] use_nag={use_nag} | models={list(model_configs.keys())} | dry_run={dry_run} | seeds={seeds_to_use}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Generate ----------\n",
        "total_generated = 0\n",
        "\n",
        "for model_name, model_config in model_configs.items():\n",
        "    steps = model_config[\"steps\"]\n",
        "    cfg = model_config[\"recommended_cfg\"]\n",
        "\n",
        "    if use_nag:\n",
        "        pipeline_cls = NAGTimeStableDiffusionXLPipeline\n",
        "        output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results-prompts-general-nag-2\"\n",
        "    else:\n",
        "        pipeline_cls = StableDiffusionXLPipeline\n",
        "        output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results-prompts-general\"\n",
        "\n",
        "    print(f\"[MODEL] {model_name} -> steps={steps}, cfg={cfg}\")\n",
        "    print(f\"[PIPELINE] {pipeline_cls.__name__}\")\n",
        "    pipe = load_pipeline(pipeline_cls, model_name)\n",
        "\n",
        "    model_output_dir = os.path.join(output_base_dir, model_name)\n",
        "\n",
        "    if use_nag:\n",
        "        baseline_dir  = os.path.join(model_output_dir, \"baseline\")\n",
        "        withneg0_dir  = os.path.join(model_output_dir, \"with_negative_start0\")\n",
        "        withneg02_dir = os.path.join(model_output_dir, \"with_negative_start0p2\")\n",
        "        if not dry_run:\n",
        "            os.makedirs(baseline_dir, exist_ok=True)\n",
        "            os.makedirs(withneg0_dir, exist_ok=True)\n",
        "            os.makedirs(withneg02_dir, exist_ok=True)\n",
        "        print(\"\\n[DIRS]\")\n",
        "        print(f\"  base_output: {output_base_dir}\")\n",
        "        print(f\"  model_root : {model_output_dir}\")\n",
        "        print(f\"  baseline   : {baseline_dir}\")\n",
        "        print(f\"  neg@0.0    : {withneg0_dir}\")\n",
        "        print(f\"  neg@0.2    : {withneg02_dir}\")\n",
        "    else:\n",
        "        baseline_dir = os.path.join(model_output_dir, \"baseline\")\n",
        "        withneg_dir  = os.path.join(model_output_dir, \"with_negative\")\n",
        "        if not dry_run:\n",
        "            os.makedirs(baseline_dir, exist_ok=True)\n",
        "            os.makedirs(withneg_dir, exist_ok=True)\n",
        "        print(\"\\n[DIRS]\")\n",
        "        print(f\"  base_output: {output_base_dir}\")\n",
        "        print(f\"  model_root : {model_output_dir}\")\n",
        "        print(f\"  baseline   : {baseline_dir}\")\n",
        "        print(f\"  with_neg   : {withneg_dir}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "    print(\"Displaying images (no writes)\\n\" if dry_run else f\"Writing under: {model_output_dir}\\n\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    generated_count = 0\n",
        "\n",
        "    for idx, item in enumerate(tqdm(prompts_data, desc=f\"{model_name} progress\")):\n",
        "        \n",
        "        prompt = \"Older woman librarian\" # item[\"prompt\"]\n",
        "        negative_prompt = \"Glasses, eyewear\" #item.get(\"negative_prompt\", \"\")\n",
        "\n",
        "        if use_nag:\n",
        "            runs = [\n",
        "                {\"out_dir\": baseline_dir,  \"neg\": False, \"nag_start\": None, \"neg_label\": \"noneg\"},\n",
        "                {\"out_dir\": withneg0_dir,  \"neg\": True,  \"nag_start\": 0.0,  \"neg_label\": \"neg_s0\"},\n",
        "                {\"out_dir\": withneg02_dir, \"neg\": True,  \"nag_start\": 0.17,  \"neg_label\": \"neg_s0p2\"},\n",
        "            ]\n",
        "\n",
        "            for run in runs:\n",
        "                for seed in seeds_to_use:\n",
        "                    try:\n",
        "                        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "                        call_kwargs = dict(\n",
        "                            prompt=prompt,\n",
        "                            guidance_scale=cfg,\n",
        "                            nag_scale = 3.0,\n",
        "                            nag_end = 0.5,\n",
        "                            nag_tau = 2.5,\n",
        "                            nag_alpha = 0.5,                            \n",
        "                            num_inference_steps=steps,\n",
        "                            generator=generator\n",
        "                        )\n",
        "\n",
        "                        call_kwargs[\"nag_negative_prompt\"] = negative_prompt if run[\"neg\"] else None\n",
        "                        call_kwargs[\"negative_prompt\"] = negative_prompt if run[\"neg\"] else None\n",
        "                        if run[\"nag_start\"] is not None:\n",
        "                            call_kwargs[\"nag_start\"] = run[\"nag_start\"]\n",
        "\n",
        "                        print(call_kwargs)\n",
        "\n",
        "                        image = pipe(**call_kwargs).images[0]\n",
        "                        filename = f\"{idx:04d}_{seed}_{run['neg_label']}.png\"\n",
        "                        filepath = os.path.join(run[\"out_dir\"], filename)\n",
        "\n",
        "                        if dry_run:\n",
        "                            print(f\"[DISPLAY] {model_name} ({run['neg_label']} | seed={seed})\")\n",
        "                            write_or_display(image, filepath, title=f\"{model_name}:{run['neg_label']}:{seed}\")\n",
        "                        else:\n",
        "                            print(f\"[WRITE] {model_name} -> {filepath}\")\n",
        "                            write_or_display(image, filepath)\n",
        "\n",
        "                        generated_count += 1\n",
        "                        total_generated += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] model={model_name}, prompt_idx={idx}, seed={seed}, run={run['neg_label']} -> {e}\")\n",
        "                        continue\n",
        "        else:\n",
        "            for use_negative in [False, True]:\n",
        "                out_dir = withneg_dir if use_negative else baseline_dir\n",
        "                neg_label = \"neg\" if use_negative else \"noneg\"\n",
        "\n",
        "                for seed in seeds_to_use:\n",
        "                    try:\n",
        "                        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "                        image = pipe(\n",
        "                            prompt,\n",
        "                            negative_prompt=negative_prompt if use_negative else None,\n",
        "                            guidance_scale=cfg,\n",
        "                            num_inference_steps=steps,\n",
        "                            generator=generator\n",
        "                        ).images[0]\n",
        "\n",
        "                        filename = f\"{idx:04d}_{seed}_{neg_label}.png\"\n",
        "                        filepath = os.path.join(out_dir, filename)\n",
        "\n",
        "                        if dry_run:\n",
        "                            print(f\"[DISPLAY] {model_name} ({neg_label} | seed={seed})\")\n",
        "                            write_or_display(image, filepath, title=f\"{model_name}:{neg_label}:{seed}\")\n",
        "                        else:\n",
        "                            print(f\"[WRITE] {model_name} -> {filepath}\")\n",
        "                            write_or_display(image, filepath)\n",
        "\n",
        "                        generated_count += 1\n",
        "                        total_generated += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] model={model_name}, prompt_idx={idx}, seed={seed}, use_negative={use_negative} -> {e}\")\n",
        "                        continue\n",
        "\n",
        "    print(f\"\\n✓ {model_name}: Generated and {'displayed' if dry_run else 'saved'} {generated_count} image(s)\")\n",
        "\n",
        "    # --- hard cleanup between models ---\n",
        "    try:\n",
        "        if 'pipe' in locals() and pipe is not None:\n",
        "            # Move weights off GPU before dropping the reference\n",
        "            try:\n",
        "                pipe.to(\"cpu\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            clear_cuda(pipe)\n",
        "        else:\n",
        "            clear_cuda()\n",
        "    finally:\n",
        "        pipe = None\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"✓ Total generated and {'displayed' if dry_run else 'saved'}: {total_generated} image(s)\")\n",
        "print(f\"✓ Models tested: {list(model_configs.keys())}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0beaab-342f-4f46-9c9f-acd0c4b0e5a3",
      "metadata": {
        "gather": {
          "logged": 1761950363658
        }
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# Benchmark latency (s/image) & peak GPU MB across NAG off/on\n",
        "# ==========================================================\n",
        "import os, time, csv, statistics, gc, json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "# from diffusers import StableDiffusionXLPipeline\n",
        "# from nag_pipeline import NAGTimeStableDiffusionXLPipeline\n",
        "# from your_module import load_pipeline\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA is required.\"\n",
        "torch.backends.cudnn.benchmark = True  # better perf on fixed shapes\n",
        "device = \"cuda\"\n",
        "\n",
        "model_configs = {\n",
        "    'base':       {'steps': 30, 'recommended_cfg': 7.0},\n",
        "    'dmd':        {'steps': 4,  'recommended_cfg': 1.5},\n",
        "    'turbo':      {'steps': 4,  'recommended_cfg': 0.0},\n",
        "    'lightning':  {'steps': 4,  'recommended_cfg': 0.0},\n",
        "    'lcm':        {'steps': 4,  'recommended_cfg': 1.5},\n",
        "    'hyper':      {'steps': 8,  'recommended_cfg': 5.0},\n",
        "    'pcm':        {'steps': 4,  'recommended_cfg': 1.0},\n",
        "}\n",
        "\n",
        "def clear_cuda(*objs):\n",
        "    \"\"\"Free refs, collect Python garbage, then flush CUDA caches.\"\"\"\n",
        "    for o in objs:\n",
        "        try:\n",
        "            del o\n",
        "        except NameError:\n",
        "            pass\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "\n",
        "clear_cuda()\n",
        "\n",
        "# ---- Config you can tweak ----\n",
        "PROMPT = \"Older woman librarian\"\n",
        "NEG_PROMPT = \"Glasses, eyewear\"\n",
        "HEIGHT, WIDTH = None, None     # set to fixed dims (e.g., 1024, 1024) for fairer comparisons\n",
        "WARMUP_RUNS, MEASURE_RUNS = 2, 5\n",
        "\n",
        "def _measure_one_image(pipe, call_kwargs):\n",
        "    \"\"\"Return (elapsed_seconds, peak_mb) for one end-to-end generation call.\"\"\"\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    _ = pipe(**call_kwargs).images[0]\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.perf_counter()\n",
        "    peak_mb = (torch.cuda.max_memory_allocated() / (1024**2)) if device == \"cuda\" else 0.0\n",
        "    return (t1 - t0), peak_mb\n",
        "\n",
        "def _run_warmup(pipe, call_kwargs, n=WARMUP_RUNS):\n",
        "    for _ in range(n):\n",
        "        _ = _measure_one_image(pipe, call_kwargs)\n",
        "\n",
        "def _techniques_for(use_nag_flag: bool):\n",
        "    if use_nag_flag:\n",
        "        return [\n",
        "            {\"label\": \"baseline\",  \"nag\": False, \"nag_start\": None},\n",
        "            {\"label\": \"neg_s0\",    \"nag\": True,  \"nag_start\": 0.0},\n",
        "            {\"label\": \"neg_s0p17\", \"nag\": True,  \"nag_start\": 0.17},\n",
        "        ]\n",
        "    else:\n",
        "        return [\n",
        "            {\"label\": \"baseline\",  \"use_negative\": False},\n",
        "            {\"label\": \"with_neg\",  \"use_negative\": True},\n",
        "        ]\n",
        "\n",
        "results = []  # rows for CSV\n",
        "total_images = 0\n",
        "\n",
        "# ================= Reordered loops: models outermost =================\n",
        "for model_name, model_cfg in model_configs.items():\n",
        "    steps = model_cfg[\"steps\"]\n",
        "    cfg   = model_cfg[\"recommended_cfg\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Running suites for model={model_name} | steps={steps}, cfg={cfg}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # For each model, run baseline first, then NAG; load once per mode, then clean up.\n",
        "    for nag_flag in [False, True]:\n",
        "        pipeline_cls = NAGTimeStableDiffusionXLPipeline if nag_flag else StableDiffusionXLPipeline\n",
        "\n",
        "        print(f\"\\n--- Building {model_name} | pipeline={pipeline_cls.__name__} ---\")\n",
        "        pipe = load_pipeline(pipeline_cls, model_name)\n",
        "        pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "        base_kwargs = dict(\n",
        "            prompt=PROMPT,\n",
        "            num_inference_steps=steps,\n",
        "            guidance_scale=cfg,\n",
        "            height=HEIGHT,\n",
        "            width=WIDTH,\n",
        "            output_type=\"pil\",\n",
        "        )\n",
        "\n",
        "        for tech in _techniques_for(nag_flag):\n",
        "            label = tech[\"label\"]\n",
        "            print(f\" -> Technique: {label}\")\n",
        "\n",
        "            call_kwargs = dict(base_kwargs)  # copy\n",
        "\n",
        "            if nag_flag:\n",
        "                if tech.get(\"nag\", False):\n",
        "                    call_kwargs.update(\n",
        "                        nag_scale=3.0,\n",
        "                        nag_end=0.5,\n",
        "                        nag_tau=2.5,\n",
        "                        nag_alpha=0.5,\n",
        "                        nag_negative_prompt=NEG_PROMPT,\n",
        "                        negative_prompt=NEG_PROMPT,\n",
        "                    )\n",
        "                    if tech[\"nag_start\"] is not None:\n",
        "                        call_kwargs[\"nag_start\"] = tech[\"nag_start\"]\n",
        "                else:\n",
        "                    call_kwargs.update(\n",
        "                        nag_scale=1.0,\n",
        "                        nag_negative_prompt=None,\n",
        "                        negative_prompt=None,\n",
        "                    )\n",
        "            else:\n",
        "                call_kwargs.update(\n",
        "                    negative_prompt=NEG_PROMPT if tech[\"use_negative\"] else None\n",
        "                )\n",
        "\n",
        "            # Warm-up runs (not recorded)\n",
        "            call_kwargs[\"generator\"] = torch.Generator(device=device).manual_seed(7)\n",
        "            _run_warmup(pipe, call_kwargs, n=WARMUP_RUNS)\n",
        "\n",
        "            # Measured runs\n",
        "            latencies, peaks = [], []\n",
        "            for i in range(MEASURE_RUNS):\n",
        "                call_kwargs[\"generator\"] = torch.Generator(device=device).manual_seed(42 + i)\n",
        "                elapsed, peak_mb = _measure_one_image(pipe, call_kwargs)\n",
        "                latencies.append(elapsed)\n",
        "                peaks.append(peak_mb)\n",
        "                total_images += 1\n",
        "                print(f\"    run {i+1}/{MEASURE_RUNS}: {elapsed:.3f}s, peak {peak_mb:.1f}MB\")\n",
        "\n",
        "            lat_mean = statistics.fmean(latencies)\n",
        "            lat_std  = statistics.pstdev(latencies) if len(latencies) > 1 else 0.0\n",
        "            mem_mean = statistics.fmean(peaks)\n",
        "            mem_std  = statistics.pstdev(peaks) if len(peaks) > 1 else 0.0\n",
        "\n",
        "            results.append({\n",
        "                \"use_nag\": nag_flag,\n",
        "                \"model\": model_name,\n",
        "                \"technique\": label,\n",
        "                \"steps\": steps,\n",
        "                \"cfg\": cfg,\n",
        "                \"runs\": MEASURE_RUNS,\n",
        "                \"latency_mean_s\": round(lat_mean, 4),\n",
        "                \"latency_std_s\": round(lat_std, 4),\n",
        "                \"peak_gpu_mb_mean\": round(mem_mean, 1),\n",
        "                \"peak_gpu_mb_std\": round(mem_std, 1),\n",
        "            })\n",
        "\n",
        "        # Cleanup between modes to keep memory stats honest\n",
        "        try:\n",
        "            pipe.to(\"cpu\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        clear_cuda(pipe)\n",
        "        pipe = None\n",
        "\n",
        "# ----- Pretty print summary -----\n",
        "from tabulate import tabulate\n",
        "print(\"\\n================ SUMMARY (both NAG off/on) ================\")\n",
        "table = []\n",
        "for row in results:\n",
        "    table.append([\n",
        "        \"NAG\" if row[\"use_nag\"] else \"no-NAG\",\n",
        "        row[\"model\"], row[\"technique\"], row[\"steps\"], row[\"cfg\"], row[\"runs\"],\n",
        "        f'{row[\"latency_mean_s\"]:.3f} ± {row[\"latency_std_s\"]:.3f}',\n",
        "        f'{row[\"peak_gpu_mb_mean\"]:.1f} ± {row[\"peak_gpu_mb_std\"]:.1f}',\n",
        "    ])\n",
        "print(tabulate(\n",
        "    table,\n",
        "    headers=[\"Mode\", \"Model\", \"Technique\", \"Steps\", \"CFG\", \"Runs\", \"Latency (s/img)\", \"Peak GPU (MB)\"],\n",
        "    tablefmt=\"github\"\n",
        "))\n",
        "\n",
        "# ----- Save one combined CSV -----\n",
        "out_csv = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/metrics_latency_memory_nag_and_baseline.csv\"\n",
        "os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
        "if results:\n",
        "    with open(out_csv, \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "    print(f\"\\nSaved metrics to: {out_csv}\\nTotal measured images: {total_images}\")\n",
        "else:\n",
        "    print(\"\\nNo results collected (unexpected).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c56b1d-e4fb-43ad-b414-c3ce684961e7",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88693443",
      "metadata": {
        "gather": {
          "logged": 1761812901245
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from PIL import Image\n",
        "import json\n",
        "import time\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "\n",
        "\n",
        "# ============ Configuration ============\n",
        "prompt_idx = 0  # Which prompt to display (0 = first)\n",
        "seed = 2047     # Which seed to use\n",
        "use_negative = False  # Whether to show \"with_negative\" or \"baseline\"\n",
        "use_nag = False  # Whether to use NAG pipeline or standard SDXL pipeline\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "#    'base': {'steps': 30, 'recommended_cfg': 7.0},\n",
        "    'dmd': {'steps': 4, 'recommended_cfg': 1.5},\n",
        "    'turbo': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lcm': {'steps': 4, 'recommended_cfg': 1.5},\n",
        "    'hyper': {'steps': 8, 'recommended_cfg': 5.0},\n",
        "    'pcm': {'steps': 4, 'recommended_cfg': 1.0},\n",
        "}\n",
        "\n",
        "\n",
        "prompt_text = \"A photo of a person\"\n",
        "negative_prompt = \"female\"\n",
        "\n",
        "\n",
        "# Model names in order\n",
        "models = ['base', 'dmd', 'turbo', 'lightning', 'lcm', 'hyper', 'pcm']\n",
        "\n",
        "print(f\"Seed: {seed} | Use Negative: {use_negative}\\n\")\n",
        "\n",
        "images = {}\n",
        "timings = {}\n",
        "\n",
        "for model_name in models:\n",
        "    if model_name not in model_configs:\n",
        "        print(f\"✗ Skipping unknown model: {model_name}\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        print(f\"[{model_name.upper()}] Loading pipeline...\", end=\" \", flush=True)\n",
        "        start_time = time.time()\n",
        "\n",
        "        if use_nag:\n",
        "            pipeline_cls = NAGTimeStableDiffusionXLPipeline\n",
        "        else:\n",
        "            pipeline_cls = StableDiffusionXLPipeline\n",
        "\n",
        "        pipe = load_pipeline(pipeline_cls, model_name)\n",
        "\n",
        "\n",
        "        load_time = time.time() - start_time\n",
        "        print(f\"done ({load_time:.2f}s)\", flush=True)\n",
        "        \n",
        "        # Get model config\n",
        "        config = model_configs[model_name]\n",
        "        steps = config['steps']\n",
        "        cfg = config['recommended_cfg']\n",
        "        \n",
        "        print(f\"  Generating with {steps} steps, CFG={cfg}...\", end=\" \", flush=True)\n",
        "        gen_start = time.time()\n",
        "        \n",
        "        # Generate image\n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        \n",
        "        if use_nag:\n",
        "            print('Nag')\n",
        "            image = pipe(\n",
        "                prompt=prompt_text,\n",
        "                guidance_scale=cfg,\n",
        "                nag_scale=3.0,\n",
        "                nag_start=0.17,\n",
        "                nag_end=0.5,\n",
        "                nag_tau=2.5,\n",
        "                nag_alpha=0.5,\n",
        "                negative_prompt=negative_prompt if use_negative else None,\n",
        "                nag_negative_prompt=negative_prompt if use_negative else None,\n",
        "                num_inference_steps=steps,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "        else:\n",
        "            print('Normal')\n",
        "            image = pipe(\n",
        "                prompt=prompt_text,\n",
        "                negative_prompt=negative_prompt if use_negative else None,\n",
        "                guidance_scale=cfg,\n",
        "                num_inference_steps=steps,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "        \n",
        "        gen_time = time.time() - gen_start\n",
        "        total_time = load_time + gen_time\n",
        "        timings[model_name] = {'load': load_time, 'gen': gen_time, 'total': total_time}\n",
        "        \n",
        "        images[model_name] = image\n",
        "        print(f\"done ({gen_time:.2f}s)\")\n",
        "        \n",
        "        # Cleanup\n",
        "        try:\n",
        "            pipe.to(\"cpu\")\n",
        "        except:\n",
        "            pass\n",
        "        pipe = None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        images[model_name] = None\n",
        "        continue\n",
        "\n",
        "# ============ Display Grid ============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Displaying generated images...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Calculate grid dimensions (try to make it roughly square)\n",
        "num_models = len([m for m in models if images.get(m) is not None])\n",
        "cols = 4\n",
        "rows = (num_models + cols - 1) // cols\n",
        "\n",
        "fig = plt.figure(figsize=(16, 4 * rows))\n",
        "gs = gridspec.GridSpec(rows, cols, figure=fig, hspace=0.3, wspace=0.2)\n",
        "\n",
        "ax_idx = 0\n",
        "for model in models:\n",
        "    if images.get(model) is not None:\n",
        "        row = ax_idx // cols\n",
        "        col = ax_idx % cols\n",
        "        ax = fig.add_subplot(gs[row, col])\n",
        "        ax.imshow(images[model])\n",
        "        \n",
        "        # Add timing info to title\n",
        "        if model in timings:\n",
        "            t = timings[model]\n",
        "            title = f\"{model.upper()}\\n({t['total']:.1f}s total)\"\n",
        "        else:\n",
        "            title = model.upper()\n",
        "        \n",
        "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "        ax_idx += 1\n",
        "\n",
        "# Hide unused subplots\n",
        "for i in range(ax_idx, rows * cols):\n",
        "    row = i // cols\n",
        "    col = i % cols\n",
        "    ax = fig.add_subplot(gs[row, col])\n",
        "    ax.axis('off')\n",
        "\n",
        "neg_label = \"WITH NEGATIVE\" if use_negative else \"BASELINE\"\n",
        "fig.suptitle(f\"Prompt {prompt_idx}: {prompt_text[:60]}...\\nSeed: {seed} | {neg_label}\", \n",
        "             fontsize=14, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✓ Grid displayed: {num_models} models\")\n",
        "print(\"\\nTiming summary:\")\n",
        "for model in models:\n",
        "    if model in timings:\n",
        "        t = timings[model]\n",
        "        print(f\"  {model:12s}: load={t['load']:6.2f}s | gen={t['gen']:6.2f}s | total={t['total']:6.2f}s\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "nag"
    },
    "kernelspec": {
      "display_name": "NAG (Python 3.13)",
      "language": "python",
      "name": "nag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
