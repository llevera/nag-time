{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9cc0d44",
      "metadata": {},
      "source": [
        "\n",
        "# Base→Student Handoff (Diversity Distillation) — *Nag‑time* compatible\n",
        "\n",
        "This notebook is a variation that **loads models the same way as _nag-time_** but implements a **custom pipeline class** (no monkey patching) to expose the right components for the diversity‑distillation base→student handoff:\n",
        "\n",
        "- `SDXLHandoffPipeline` (subclass of `StableDiffusionXLPipeline`) exposes:\n",
        "  - `from_timestep`, `till_timestep`\n",
        "  - `start_latents`\n",
        "  - `output_type=\"latent\"` support for returning raw latents\n",
        "- `diversity_distillation(...)` runs base UNet for the first few steps, hands latents to the distilled UNet, and completes generation.\n",
        "\n",
        "> **Note:** This notebook avoids `StableDiffusionXLPipeline.__call__ = ...` and uses a new class instead, matching the requirement \"same as Nag time, not monkey patched\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c9b1b60",
      "metadata": {
        "gather": {
          "logged": 1761950639977
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Core libs\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Diffusers / Transformers\n",
        "from diffusers import (\n",
        "    DiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        "    LCMScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    TCDScheduler,\n",
        ")\n",
        "from diffusers.pipelines import StableDiffusionXLPipeline\n",
        "from diffusers.pipelines.pipeline_utils import ImagePipelineOutput\n",
        "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import retrieve_timesteps\n",
        "from diffusers.image_processor import PipelineImageInput, VaeImageProcessor\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98bca256",
      "metadata": {
        "gather": {
          "logged": 1761950641629
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def rescale_noise_cfg(noise_cfg: torch.Tensor, noise_pred_text: torch.Tensor, guidance_rescale: float = 0.0):\n",
        "    \"\"\"Rescales guidance to improve image quality (fixes overexposure).\n",
        "    Based on Section 3.4 of 'Common Diffusion Noise Schedules and Sample Steps are Flawed' (2305.08891).\n",
        "    \"\"\"\n",
        "    std_text = noise_pred_text.std(dim=list(range(1, noise_pred_text.ndim)), keepdim=True)\n",
        "    std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)\n",
        "    noise_pred_rescaled = noise_cfg * (std_text / std_cfg)\n",
        "    noise_cfg = guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg\n",
        "    return noise_cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66450caa",
      "metadata": {
        "gather": {
          "logged": 1761951208897
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "class SDXLHandoffPipeline(StableDiffusionXLPipeline):\n",
        "    \"\"\"Custom SDXL pipeline exposing start/end timesteps and latent handoff without monkey patching.\"\"\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        timesteps: List[int] = None,\n",
        "        sigmas: List[float] = None,\n",
        "        denoising_end: Optional[float] = None,\n",
        "        guidance_scale: float = 5.0,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: Optional[int] = 1,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.Tensor] = None,\n",
        "        prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
        "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        guidance_rescale: float = 0.0,\n",
        "        original_size: Optional[Tuple[int, int]] = None,\n",
        "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        target_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
        "        clip_skip: Optional[int] = None,\n",
        "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
        "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
        "        from_timestep: int = 0,\n",
        "        till_timestep: Optional[int] = None,\n",
        "        start_latents: Optional[torch.Tensor] = None,\n",
        "    ) -> Union[ImagePipelineOutput, List[torch.Tensor], torch.Tensor]:\n",
        "        \"\"\"A lightly adapted SDXL __call__ that supports partial denoising and latent handoff.\n",
        "        \n",
        "        Key additions:\n",
        "        - from_timestep / till_timestep to run a sub-trajectory\n",
        "        - start_latents: start from provided latent state (for handoff)\n",
        "        - output_type='latent' to return raw latents\n",
        "        \"\"\"\n",
        "        # 0) Defaults and bookkeeping\n",
        "        height = height or self.default_sample_size * self.vae_scale_factor\n",
        "        width = width or self.default_sample_size * self.vae_scale_factor\n",
        "        original_size = original_size or (height, width)\n",
        "        target_size = target_size or (height, width)\n",
        "\n",
        "        # 1) Check inputs\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            prompt_2,\n",
        "            height,\n",
        "            width,\n",
        "            None,  # callback_steps (unused here)\n",
        "            negative_prompt,\n",
        "            negative_prompt_2,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "            ip_adapter_image,\n",
        "            ip_adapter_image_embeds,\n",
        "            callback_on_step_end_tensor_inputs,\n",
        "        )\n",
        "\n",
        "        self._guidance_scale = guidance_scale\n",
        "        self._guidance_rescale = guidance_rescale\n",
        "        self._clip_skip = clip_skip\n",
        "        self._cross_attention_kwargs = cross_attention_kwargs\n",
        "        self._denoising_end = denoising_end\n",
        "        self._interrupt = False\n",
        "\n",
        "        # 2) Batch size\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        # 3) Encode prompts\n",
        "        lora_scale = self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
        "\n",
        "        (\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        ) = self.encode_prompt(\n",
        "            prompt=prompt,\n",
        "            prompt_2=prompt_2,\n",
        "            device=device,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_2=negative_prompt_2,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
        "            lora_scale=lora_scale,\n",
        "            clip_skip=self.clip_skip,\n",
        "        )\n",
        "\n",
        "        # 4) Timesteps\n",
        "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps, sigmas)\n",
        "\n",
        "        # 5) Latents\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_images_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            width,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "\n",
        "        # 6) Extra step kwargs\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # 7) Added time ids & embeddings\n",
        "        add_text_embeds = pooled_prompt_embeds\n",
        "        if self.text_encoder_2 is None:\n",
        "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
        "        else:\n",
        "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
        "\n",
        "        add_time_ids = self._get_add_time_ids(\n",
        "            original_size,\n",
        "            crops_coords_top_left,\n",
        "            target_size,\n",
        "            dtype=prompt_embeds.dtype,\n",
        "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
        "        )\n",
        "        if negative_original_size is not None and negative_target_size is not None:\n",
        "            negative_add_time_ids = self._get_add_time_ids(\n",
        "                negative_original_size,\n",
        "                negative_crops_coords_top_left,\n",
        "                negative_target_size,\n",
        "                dtype=prompt_embeds.dtype,\n",
        "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
        "            )\n",
        "        else:\n",
        "            negative_add_time_ids = add_time_ids\n",
        "\n",
        "        if self.do_classifier_free_guidance:\n",
        "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
        "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
        "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
        "\n",
        "        prompt_embeds = prompt_embeds.to(device)\n",
        "        add_text_embeds = add_text_embeds.to(device)\n",
        "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
        "\n",
        "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
        "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
        "                ip_adapter_image,\n",
        "                ip_adapter_image_embeds,\n",
        "                device,\n",
        "                batch_size * num_images_per_prompt,\n",
        "                self.do_classifier_free_guidance,\n",
        "            )\n",
        "\n",
        "        # 8) Denoising loop setup\n",
        "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
        "\n",
        "        if (self.denoising_end is not None and isinstance(self.denoising_end, float) \n",
        "                and 0 < self.denoising_end < 1):\n",
        "            discrete_timestep_cutoff = int(round(self.scheduler.config.num_train_timesteps - (self.denoising_end * self.scheduler.config.num_train_timesteps)))\n",
        "            num_inference_steps = len([ts for ts in timesteps if ts >= discrete_timestep_cutoff])\n",
        "            timesteps = timesteps[:num_inference_steps]\n",
        "\n",
        "        timestep_cond = None\n",
        "        if self.unet.config.time_cond_proj_dim is not None:\n",
        "            guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
        "            timestep_cond = self.get_guidance_scale_embedding(\n",
        "                guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
        "            ).to(device=device, dtype=latents.dtype)\n",
        "\n",
        "        self._num_timesteps = len(timesteps)\n",
        "\n",
        "        # Handoff support\n",
        "        if start_latents is not None:\n",
        "            latents = start_latents.to(device=device, dtype=latents.dtype)\n",
        "\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps[from_timestep:till_timestep]):\n",
        "                # Prepare input (CFG)\n",
        "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # Predict noise\n",
        "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
        "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
        "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
        "\n",
        "                noise_pred = self.unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    timestep_cond=timestep_cond,\n",
        "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
        "                    added_cond_kwargs=added_cond_kwargs,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "\n",
        "                # CFG\n",
        "                if self.do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
        "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
        "\n",
        "                # Step\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
        "\n",
        "                # Progress\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "\n",
        "        # Decode or return latents\n",
        "        if output_type == \"latent\":\n",
        "            image = latents\n",
        "        else:\n",
        "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
        "            if needs_upcasting:\n",
        "                self.upcast_vae()\n",
        "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
        "            elif latents.dtype != self.vae.dtype and torch.backends.mps.is_available():\n",
        "                self.vae = self.vae.to(latents.dtype)\n",
        "\n",
        "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
        "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
        "            if has_latents_mean and has_latents_std:\n",
        "                latents_mean = torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
        "                latents_std = torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
        "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
        "            else:\n",
        "                latents = latents / self.vae.config.scaling_factor\n",
        "\n",
        "            image = self.vae.decode(latents, return_dict=False)[0]\n",
        "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
        "\n",
        "        self.maybe_free_model_hooks()\n",
        "\n",
        "        if not return_dict:\n",
        "            return image\n",
        "        return ImagePipelineOutput(images=image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd5b03e",
      "metadata": {
        "gather": {
          "logged": 1761951254264
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def diversity_distillation(\n",
        "    prompt: str,\n",
        "    negative_prompt: str,\n",
        "    seed: int,\n",
        "    pipe: SDXLHandoffPipeline,\n",
        "    base_unet: UNet2DConditionModel,\n",
        "    distilled_unet: UNet2DConditionModel,\n",
        "    distilled_scheduler,\n",
        "    base_guidance_scale: float = 5.0,\n",
        "    distilled_guidance_scale: float = 0.0,\n",
        "    num_inference_steps: int = 4,\n",
        "    run_base_till: int = 1,\n",
        "    output_type: str = \"pil\",\n",
        "):\n",
        "    \"\"\"Run base for a few steps, then hand off to distilled UNet to finish.\n",
        "    \n",
        "    Returns PIL images (default) or raw latents if `output_type='latent'`.\n",
        "    \"\"\"\n",
        "    # 1) Run the base to get partial latents\n",
        "    pipe.scheduler = distilled_scheduler  # schedule shape should match handoff trajectory\n",
        "    pipe.unet = base_unet\n",
        "    base_latents = pipe(\n",
        "        prompt,\n",
        "        guidance_scale=base_guidance_scale,\n",
        "        till_timestep=run_base_till,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        generator=torch.Generator(device=pipe.device).manual_seed(seed),\n",
        "        output_type='latent',\n",
        "        return_dict=False,\n",
        "    )\n",
        "\n",
        "    # 2) Continue from those latents with the distilled student\n",
        "    pipe.unet = distilled_unet\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        guidance_scale=distilled_guidance_scale,\n",
        "        start_latents=base_latents,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        from_timestep=run_base_till,\n",
        "        output_type=output_type,\n",
        "        return_dict=False,\n",
        "    )\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c2f3bb",
      "metadata": {
        "gather": {
          "logged": 1761951208929
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_model(distillation_type: Optional[str] = None, weights_dtype=torch.bfloat16, device: str = 'cuda:0'):\n",
        "    \"\"\"Load SDXL models with specified distillation type.\n",
        "    \n",
        "    For 'base': returns (pipe, base_unet, base_scheduler)\n",
        "    For others: returns (pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler)\n",
        "    \"\"\"\n",
        "    basemodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "    # Base model and scheduler\n",
        "    base_unet = UNet2DConditionModel.from_pretrained(basemodel_id, subfolder=\"unet\").to(device, weights_dtype)\n",
        "    pipe = SDXLHandoffPipeline.from_pretrained(basemodel_id, unet=base_unet, torch_dtype=weights_dtype, use_safetensors=True)\n",
        "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    base_scheduler = pipe.scheduler\n",
        "\n",
        "    if distillation_type is None:\n",
        "        pipe.to(device).to(weights_dtype)\n",
        "        return pipe, base_unet, base_scheduler\n",
        "\n",
        "    # Distilled variants\n",
        "    if distillation_type == 'dmd':\n",
        "        repo_name = \"tianweiy/DMD2\"\n",
        "        ckpt_name = \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
        "        distilled_unet = UNet2DConditionModel.from_config(basemodel_id, subfolder=\"unet\").to(device, weights_dtype)\n",
        "        distilled_unet.load_state_dict(torch.load(hf_hub_download(repo_name, ckpt_name), weights_only=True))\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif distillation_type == 'lightning':\n",
        "        repo = \"ByteDance/SDXL-Lightning\"\n",
        "        ckpt = \"sdxl_lightning_4step_unet.safetensors\"\n",
        "        distilled_unet = UNet2DConditionModel.from_config(basemodel_id, subfolder=\"unet\").to(device, weights_dtype)\n",
        "        distilled_unet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=device))\n",
        "        distilled_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "        base_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif distillation_type == 'turbo':\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\"stabilityai/sdxl-turbo\", subfolder=\"unet\", torch_dtype=weights_dtype, variant=\"fp16\").to(device, weights_dtype)\n",
        "        distilled_scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "        base_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif distillation_type == 'lcm':\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\"latent-consistency/lcm-sdxl\", torch_dtype=weights_dtype).to(device, weights_dtype)\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif distillation_type == 'hyper':\n",
        "        # Hyper-SDXL 8-step CFG-preserved LoRA\n",
        "        pipe_tmp = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe_tmp.load_lora_weights(\"ByteDance/Hyper-SD\", weight_name=\"Hyper-SDXL-8steps-CFG-lora.safetensors\", adapter_name=\"hyper-sdxl-8step\")\n",
        "        pipe_tmp.set_adapters([\"hyper-sdxl-8step\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe_tmp.unet\n",
        "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif distillation_type == 'hyper_1step':\n",
        "        pipe_tmp = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe_tmp.load_lora_weights(\"ByteDance/Hyper-SD\", weight_name=\"Hyper-SDXL-1step-lora.safetensors\", adapter_name=\"hyper-sdxl-1step\")\n",
        "        pipe_tmp.set_adapters([\"hyper-sdxl-1step\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe_tmp.unet\n",
        "        distilled_scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif distillation_type == 'pcm':\n",
        "        pipe_tmp = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe_tmp.load_lora_weights(\"wangfuyun/PCM_Weights\", weight_name=\"pcm_sdxl_smallcfg_4step_converted.safetensors\", subfolder=\"sdxl\", adapter_name=\"pcm-lora\")\n",
        "        pipe_tmp.set_adapters([\"pcm-lora\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe_tmp.unet\n",
        "        distilled_scheduler = DDIMScheduler.from_config(\n",
        "            pipe.scheduler.config,\n",
        "            timestep_spacing=\"trailing\",\n",
        "            clip_sample=False,\n",
        "            set_alpha_to_one=False,\n",
        "        )\n",
        "\n",
        "    elif distillation_type == 'tcd':\n",
        "        pipe_tmp = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe_tmp.load_lora_weights(\"h1t/TCD-SDXL-LoRA\", adapter_name=\"tcd-lora\")\n",
        "        pipe_tmp.set_adapters([\"tcd-lora\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe_tmp.unet\n",
        "        distilled_scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif distillation_type == 'flash':\n",
        "        repo = \"jasperai/flash-sdxl\"\n",
        "        ckpt = \"pytorch_lora_weights.safetensors\"\n",
        "        pipe_tmp = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe_tmp.load_lora_weights(repo, weight_name=ckpt, adapter_name=\"flash-sdxl\")\n",
        "        pipe_tmp.set_adapters([\"flash-sdxl\"], adapter_weights=[1.0])\n",
        "        pipe_tmp.fuse_lora()\n",
        "        distilled_unet = pipe_tmp.unet\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown distillation type: {distillation_type}\")\n",
        "\n",
        "    # Finalise device / dtype\n",
        "    \n",
        "    pipe.to(device).to(weights_dtype)\n",
        "\n",
        "    if distillation_type is not None:\n",
        "        distilled_unet = distilled_unet.to(device, dtype=weights_dtype)\n",
        "    return pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
        "\n",
        "\n",
        "def load_pipe(distillation_type: Optional[str] = None, weights_dtype=torch.bfloat16, device: str = 'cuda:0'):\n",
        "    \"\"\"Return a configured SDXL handoff pipeline for direct use (mirrors the nag-time loader shape).\"\"\"\n",
        "    result = load_model(distillation_type, weights_dtype, device)\n",
        "    if distillation_type is None:\n",
        "        pipe, _, _ = result\n",
        "    else:\n",
        "        pipe, _, _, _, distilled_scheduler = result\n",
        "        pipe.scheduler = distilled_scheduler\n",
        "    return pipe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34d1991",
      "metadata": {},
      "source": [
        "\n",
        "## Minimal usage\n",
        "\n",
        "Uncomment and run to test. This keeps defaults very light (4 steps) for quick checks, but you can scale up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd20867",
      "metadata": {
        "gather": {
          "logged": 1761951214660
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example (commented out to avoid heavy downloads/execution here):\n",
        "device = 'cuda:0' \n",
        "pipe, base_unet, base_sched, distilled_unet, distilled_sched = load_model('turbo', device=device)\n",
        "img = diversity_distillation(\n",
        "     prompt=\"a older librarian\",\n",
        "     negative_prompt=\"male\",\n",
        "     seed=44,\n",
        "     pipe=pipe,\n",
        "     base_unet=base_unet,\n",
        "     distilled_unet=distilled_unet,\n",
        "     distilled_scheduler=distilled_sched,\n",
        "     base_guidance_scale=5.0,\n",
        "     distilled_guidance_scale=1.0,\n",
        "     num_inference_steps=4,\n",
        "     run_base_till=1,\n",
        ")\n",
        "img[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2feea4e-e1b5-48b1-948a-25add5750b94",
      "metadata": {
        "gather": {
          "logged": 1761818313172
        }
      },
      "outputs": [],
      "source": [
        "# --- Diversity Distillation: negatives vs not (full test cell) ---\n",
        "\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display as notebook_display\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# ============== Config ==============\n",
        "dry_run = False  # When True: process only 1 prompt and display images instead of saving\n",
        "\n",
        "# e.g. any JSON list of {\"prompt\": \"...\", \"negative_prompt\": \"...\", \"score\": <optional>}\n",
        "prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_general.json\"\n",
        "\n",
        "# Distilled model variants and suggested guidance settings for STUDENT phase.\n",
        "# Base phase CFG is typically ~5.0; student CFG varies by distillation type.\n",
        "model_configs = {\n",
        "    'dmd':       {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 1.5},\n",
        "    'turbo':     {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 0.0},\n",
        "    'lcm':       {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 1.5},\n",
        "    'hyper':     {'steps': 8, 'base_cfg': 7.0, 'student_cfg': 5.0},\n",
        "    'pcm':       {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 1.0},\n",
        "}\n",
        "\n",
        "# Handoff settings\n",
        "dd_run_base_till = 1  # run the base UNet for this many steps, then hand off to the student\n",
        "\n",
        "# Fixed seeds\n",
        "fixed_seeds = [2025, 42, 1337]\n",
        "\n",
        "# Output location\n",
        "output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results-dd-neg-compare\"\n",
        "\n",
        "# ===================================\n",
        "\n",
        "def clear_cuda(*objs):\n",
        "    \"\"\"Free refs, collect Python garbage, then flush CUDA caches.\"\"\"\n",
        "    for o in objs:\n",
        "        try:\n",
        "            del o\n",
        "        except NameError:\n",
        "            pass\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "def write_or_display(image, filepath, title=\"preview\"):\n",
        "    if dry_run:\n",
        "        notebook_display(image)\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        image.save(filepath)\n",
        "\n",
        "# Load prompts\n",
        "with open(prompts_file, 'r') as f:\n",
        "    prompts_data = json.load(f)\n",
        "\n",
        "# Select top 50 by \"score\" if present, else first 50\n",
        "if isinstance(prompts_data, list) and len(prompts_data) > 0 and isinstance(prompts_data[0], dict) and 'score' in prompts_data[0]:\n",
        "    prompts_data = sorted(prompts_data, key=lambda x: x.get('score', 0), reverse=True)[:50]\n",
        "else:\n",
        "    prompts_data = prompts_data[:50]\n",
        "\n",
        "if dry_run:\n",
        "    prompts_data = prompts_data[:1]  # only one prompt\n",
        "\n",
        "# Choose seeds (all vs one when dry-run)\n",
        "seeds_to_use = fixed_seeds[:1] if dry_run else fixed_seeds\n",
        "\n",
        "print(f\"Loaded {len(prompts_data)} prompt(s) from {prompts_file}\")\n",
        "print(f\"[CONFIG] models={list(model_configs.keys())} | dry_run={dry_run} | seeds={seeds_to_use}\")\n",
        "print(f\"[CONFIG] dd_run_base_till={dd_run_base_till}\\n\")\n",
        "\n",
        "# ============== Generate ==============\n",
        "total_generated = 0\n",
        "\n",
        "for model_name, cfgs in model_configs.items():\n",
        "    steps      = cfgs[\"steps\"]\n",
        "    base_cfg   = cfgs[\"base_cfg\"]\n",
        "    student_cfg= cfgs[\"student_cfg\"]\n",
        "\n",
        "    print(f\"[MODEL] {model_name} -> steps={steps}, base_cfg={base_cfg}, student_cfg={student_cfg}\")\n",
        "\n",
        "    # Load pipe + components for this distilled model\n",
        "    try:\n",
        "        load_result = load_model(model_name, device=(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to load model {model_name}: {e}\")\n",
        "        clear_cuda()\n",
        "        continue\n",
        "\n",
        "    # Unpack result (4-tuple vs 5-tuple depending on loader; for distilled we expect 5)\n",
        "    try:\n",
        "        pipe, base_unet, base_sched, distilled_unet, distilled_sched = load_result\n",
        "    except ValueError:\n",
        "        # If the loader returns only base pieces, skip (not relevant to this comparison)\n",
        "        print(f\"[WARN] Loader did not return distilled components for {model_name}; skipping.\")\n",
        "        clear_cuda()\n",
        "        continue\n",
        "\n",
        "    model_output_dir = os.path.join(output_base_dir, model_name)\n",
        "    noneg_dir  = os.path.join(model_output_dir, \"dd_noneg\")\n",
        "    withneg_dir= os.path.join(model_output_dir, \"dd_withneg\")\n",
        "\n",
        "    if not dry_run:\n",
        "        os.makedirs(noneg_dir, exist_ok=True)\n",
        "        os.makedirs(withneg_dir, exist_ok=True)\n",
        "\n",
        "    print(\"\\n[DIRS]\")\n",
        "    print(f\"  base_output: {output_base_dir}\")\n",
        "    print(f\"  model_root : {model_output_dir}\")\n",
        "    print(f\"  noneg      : {noneg_dir}\")\n",
        "    print(f\"  withneg    : {withneg_dir}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing diversity distillation (negatives vs not): {model_name}\")\n",
        "    print(\"Displaying images (no writes)\\n\" if dry_run else f\"Writing under: {model_output_dir}\\n\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    generated_count = 0\n",
        "\n",
        "    for idx, item in enumerate(tqdm(prompts_data, desc=f\"{model_name} progress\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "        neg = item.get(\"negative_prompt\", None)\n",
        "\n",
        "        runs = [\n",
        "            {\"out_dir\": noneg_dir,   \"use_negative\": False, \"label\": \"noneg\"},\n",
        "            {\"out_dir\": withneg_dir, \"use_negative\": True,  \"label\": \"withneg\"},\n",
        "        ]\n",
        "\n",
        "        for run in runs:\n",
        "            for seed in seeds_to_use:\n",
        "                try:\n",
        "                    # Call diversity_distillation twice: without negatives vs with negatives.\n",
        "                    # The function builds its own torch.Generator from the seed & pipe.device.\n",
        "                    images = diversity_distillation(\n",
        "                        prompt=prompt,\n",
        "                        seed=seed,\n",
        "                        pipe=pipe,\n",
        "                        base_unet=base_unet,\n",
        "                        distilled_unet=distilled_unet,\n",
        "                        distilled_scheduler=distilled_sched,\n",
        "                        base_guidance_scale=base_cfg,\n",
        "                        distilled_guidance_scale=student_cfg,\n",
        "                        num_inference_steps=steps,\n",
        "                        run_base_till=dd_run_base_till,\n",
        "                        output_type=\"pil\",\n",
        "                        negative_prompt=(neg if run[\"use_negative\"] else None)\n",
        "                    )\n",
        "\n",
        "                    # diversity_distillation returns a list/np array of images, or a single image depending on pipeline\n",
        "                    image = images[0] if isinstance(images, (list, tuple)) else images\n",
        "                    filename = f\"{idx:04d}_{seed}_{run['label']}.png\"\n",
        "                    filepath = os.path.join(run[\"out_dir\"], filename)\n",
        "\n",
        "                    if dry_run:\n",
        "                        print(f\"[DISPLAY] {model_name} ({run['label']} | seed={seed})\")\n",
        "                        write_or_display(image, filepath, title=f\"{model_name}:{run['label']}:{seed}\")\n",
        "                    else:\n",
        "                        print(f\"[WRITE] {model_name} -> {filepath}\")\n",
        "                        write_or_display(image, filepath)\n",
        "\n",
        "                    generated_count += 1\n",
        "                    total_generated += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] model={model_name}, prompt_idx={idx}, seed={seed}, run={run['label']} -> {e}\")\n",
        "                    continue\n",
        "\n",
        "    print(f\"\\n✓ {model_name}: Generated and {'displayed' if dry_run else 'saved'} {generated_count} image(s)\")\n",
        "\n",
        "    # --- hard cleanup between models ---\n",
        "    try:\n",
        "        if 'pipe' in locals() and pipe is not None:\n",
        "            try:\n",
        "                pipe.to(\"cpu\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            clear_cuda(pipe, base_unet, distilled_unet)\n",
        "        else:\n",
        "            clear_cuda()\n",
        "    finally:\n",
        "        pipe = None\n",
        "        base_unet = None\n",
        "        distilled_unet = None\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"✓ Total generated and {'displayed' if dry_run else 'saved'}: {total_generated} image(s)\")\n",
        "print(f\"✓ Models tested: {list(model_configs.keys())}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "570c0823-a292-4748-a9ea-9ffe7a9a1388",
      "metadata": {
        "gather": {
          "logged": 1761951393442
        }
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# Diversity Distillation benchmark: latency (s/image) & peak GPU MB\n",
        "# Techniques: dd_noneg vs dd_withneg for each model in model_configs\n",
        "# ==========================================================\n",
        "import json, os, time, csv, statistics, gc\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch  # <-- ensure torch is imported\n",
        "\n",
        "assert torch.cuda.is_available(), \"CUDA is required for this benchmark.\"\n",
        "torch.backends.cudnn.benchmark = True  # faster kernels with fixed shapes\n",
        "device = \"cuda\"\n",
        "\n",
        "# Handoff settings\n",
        "dd_run_base_till = 1  # run the base UNet for this many steps, then hand off to the student\n",
        "\n",
        "# Fixed seeds\n",
        "fixed_seeds = [2025, 42, 1337]\n",
        "\n",
        "\n",
        "prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_general.json\"\n",
        "\n",
        "# Distilled model variants and suggested guidance settings for STUDENT phase.\n",
        "# Base phase CFG is typically ~5.0; student CFG varies by distillation type.\n",
        "model_configs = {\n",
        "    'dmd':       {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 1.5},\n",
        "    'turbo':     {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 0.0},\n",
        "    'lcm':       {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 1.5},\n",
        "    'hyper':     {'steps': 8, 'base_cfg': 7.0, 'student_cfg': 5.0},\n",
        "    'pcm':       {'steps': 4, 'base_cfg': 7.0, 'student_cfg': 1.0},\n",
        "}\n",
        "\n",
        "# --------- You can tweak these ---------\n",
        "WARMUP_RUNS   = 2\n",
        "MEASURE_RUNS  = 5   # per your requirement: >= 5 after warm-up\n",
        "HEIGHT        = None   # keep None to use pipeline defaults; set e.g. 1024 for apples-to-apples\n",
        "WIDTH         = None\n",
        "OUT_CSV = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results-dd-neg-compare/metrics_dd_latency_memory.csv\"\n",
        "# --------------------------------------\n",
        "\n",
        "def _clear_cuda(*objs):\n",
        "    for o in objs:\n",
        "        try:\n",
        "            del o\n",
        "        except NameError:\n",
        "            pass\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "def _measure_one_call(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Measure end-to-end elapsed seconds (per image) and CUDA peak MB for one call.\n",
        "    Returns (seconds_per_image, peak_mb, out).\n",
        "    \"\"\"\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "    out = func(*args, **kwargs)\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    # Determine number of images returned to compute seconds/image\n",
        "    if isinstance(out, (list, tuple)):\n",
        "        n_imgs = len(out) if len(out) > 0 else 1\n",
        "    else:\n",
        "        n_imgs = 1\n",
        "    elapsed = (t1 - t0) / max(1, n_imgs)\n",
        "\n",
        "    peak_mb = (torch.cuda.max_memory_allocated() / (1024**2)) if device == \"cuda\" else 0.0\n",
        "    return elapsed, peak_mb, out\n",
        "\n",
        "# ---- Load prompts (reusing your config above) ----\n",
        "with open(prompts_file, 'r') as f:\n",
        "    prompts_data = json.load(f)\n",
        "if isinstance(prompts_data, list) and len(prompts_data) > 0 and isinstance(prompts_data[0], dict) and 'score' in prompts_data[0]:\n",
        "    prompts_sorted = sorted(prompts_data, key=lambda x: x.get('score', 0), reverse=True)\n",
        "else:\n",
        "    prompts_sorted = prompts_data\n",
        "\n",
        "# We'll benchmark using a single prompt to keep the measurement tight & repeatable.\n",
        "bench_item = prompts_sorted[0]\n",
        "BENCH_PROMPT = bench_item[\"prompt\"]\n",
        "BENCH_NEG    = bench_item.get(\"negative_prompt\", \"\")\n",
        "\n",
        "print(f\"[BENCHMARK PROMPT]\\n  prompt: {BENCH_PROMPT[:100]}{'...' if len(BENCH_PROMPT)>100 else ''}\\n  negative_prompt: {BENCH_NEG[:100]}{'...' if BENCH_NEG and len(BENCH_NEG)>100 else ''}\")\n",
        "\n",
        "results = []\n",
        "total_measured_images = 0\n",
        "\n",
        "# ============= Models outermost; techniques inner; hard cleanup per model =============\n",
        "for model_name, cfgs in model_configs.items():\n",
        "    steps       = cfgs[\"steps\"]\n",
        "    base_cfg    = cfgs[\"base_cfg\"]\n",
        "    student_cfg = cfgs[\"student_cfg\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Model: {model_name} | steps={steps} | base_cfg={base_cfg} | student_cfg={student_cfg}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load once per model\n",
        "    try:\n",
        "        load_result = load_model(model_name, device=(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to load model {model_name}: {e}\")\n",
        "        _clear_cuda()\n",
        "        continue\n",
        "\n",
        "    # Expect distilled components (pipe, base_unet, base_sched, distilled_unet, distilled_sched)\n",
        "    try:\n",
        "        pipe, base_unet, base_sched, distilled_unet, distilled_sched = load_result\n",
        "    except ValueError:\n",
        "        print(f\"[WARN] Loader did not return distilled components for {model_name}; skipping.\")\n",
        "        _clear_cuda()\n",
        "        continue\n",
        "\n",
        "    techniques = [\n",
        "        {\"label\": \"dd_noneg\",   \"use_negative\": False},\n",
        "        {\"label\": \"dd_withneg\", \"use_negative\": True},\n",
        "    ]\n",
        "\n",
        "    # Common kwargs to diversity_distillation\n",
        "    base_kwargs = dict(\n",
        "        pipe=pipe,\n",
        "        base_unet=base_unet,\n",
        "        distilled_unet=distilled_unet,\n",
        "        distilled_scheduler=distilled_sched,\n",
        "        base_guidance_scale=base_cfg,\n",
        "        distilled_guidance_scale=student_cfg,\n",
        "        num_inference_steps=steps,\n",
        "        run_base_till=dd_run_base_till,\n",
        "        output_type=\"pil\",\n",
        "        # height=HEIGHT,  # uncomment if your function/plumbing supports explicit size\n",
        "        # width=WIDTH,\n",
        "    )\n",
        "\n",
        "    for tech in techniques:\n",
        "        label = tech[\"label\"]\n",
        "        print(f\"\\n -> Technique: {label}\")\n",
        "\n",
        "        # Warm-up (not recorded)\n",
        "        for _ in range(WARMUP_RUNS):\n",
        "            _measure_one_call(\n",
        "                diversity_distillation,\n",
        "                prompt=BENCH_PROMPT,\n",
        "                seed=1234,\n",
        "                negative_prompt=(BENCH_NEG if tech[\"use_negative\"] else None),\n",
        "                **base_kwargs\n",
        "            )\n",
        "\n",
        "        # Measured runs\n",
        "        latencies, peaks = [], []\n",
        "        for i in range(MEASURE_RUNS):\n",
        "            elapsed, peak_mb, _ = _measure_one_call(\n",
        "                diversity_distillation,\n",
        "                prompt=BENCH_PROMPT,\n",
        "                seed=42 + i,  # vary to avoid cache coincidences\n",
        "                negative_prompt=(BENCH_NEG if tech[\"use_negative\"] else None),\n",
        "                **base_kwargs\n",
        "            )\n",
        "            latencies.append(elapsed)\n",
        "            peaks.append(peak_mb)\n",
        "            total_measured_images += 1\n",
        "            print(f\"    run {i+1}/{MEASURE_RUNS}: {elapsed:.3f}s per image, peak {peak_mb:.1f}MB\")\n",
        "\n",
        "        lat_mean = statistics.fmean(latencies)\n",
        "        lat_std  = statistics.pstdev(latencies) if len(latencies) > 1 else 0.0\n",
        "        mem_mean = statistics.fmean(peaks)\n",
        "        mem_std  = statistics.pstdev(peaks) if len(peaks) > 1 else 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"model\": model_name,\n",
        "            \"technique\": label,\n",
        "            \"steps\": steps,\n",
        "            \"base_cfg\": base_cfg,\n",
        "            \"student_cfg\": student_cfg,\n",
        "            \"runs\": MEASURE_RUNS,\n",
        "            \"latency_mean_s_per_image\": round(lat_mean, 4),\n",
        "            \"latency_std_s_per_image\": round(lat_std, 4),\n",
        "            \"peak_gpu_mb_mean\": round(mem_mean, 1),\n",
        "            \"peak_gpu_mb_std\": round(mem_std, 1),\n",
        "        })\n",
        "\n",
        "    # Hard cleanup between models (keep stats honest)\n",
        "    try:\n",
        "        pipe.to(\"cpu\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    _clear_cuda(pipe, base_unet, distilled_unet, base_sched, distilled_sched)\n",
        "    pipe = base_unet = distilled_unet = base_sched = distilled_sched = None\n",
        "\n",
        "# --------- Pretty print summary ----------\n",
        "from tabulate import tabulate\n",
        "print(\"\\n================ SUMMARY: Diversity Distillation (neg vs not) ================\")\n",
        "table = []\n",
        "for row in results:\n",
        "    table.append([\n",
        "        row[\"model\"], row[\"technique\"], row[\"steps\"],\n",
        "        row[\"base_cfg\"], row[\"student_cfg\"], row[\"runs\"],\n",
        "        f'{row[\"latency_mean_s_per_image\"]:.3f} ± {row[\"latency_std_s_per_image\"]:.3f}',\n",
        "        f'{row[\"peak_gpu_mb_mean\"]:.1f} ± {row[\"peak_gpu_mb_std\"]:.1f}',\n",
        "    ])\n",
        "print(tabulate(\n",
        "    table,\n",
        "    headers=[\"Model\", \"Technique\", \"Steps\", \"Base CFG\", \"Student CFG\", \"Runs\",\n",
        "             \"Latency (s/image)\", \"Peak GPU (MB)\"],\n",
        "    tablefmt=\"github\"\n",
        "))\n",
        "\n",
        "# --------- Save CSV ----------\n",
        "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
        "if results:\n",
        "    with open(OUT_CSV, \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "    print(f\"\\nSaved metrics to: {OUT_CSV}\\nTotal measured images: {total_measured_images}\")\n",
        "else:\n",
        "    print(\"\\nNo results collected (unexpected).\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "nag"
    },
    "kernelspec": {
      "display_name": "NAG (Python 3.13)",
      "language": "python",
      "name": "nag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
