{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ef94728",
      "metadata": {},
      "source": [
        "# SDXL Model Pipeline Setup - Lightning Fix Applied\n",
        "Supports 9 distillation models with proper scheduler configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "02129d5a",
      "metadata": {
        "gather": {
          "logged": 1761377794022
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports completed\n",
            "✓ Configuration set - Device: cuda, Dtype: torch.bfloat16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/lib/python3.13/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n"
          ]
        }
      ],
      "source": [
        "# Imports and Configuration\n",
        "import sys\n",
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import (\n",
        "    UNet2DConditionModel,\n",
        "    StableDiffusionXLPipeline,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    LCMScheduler,\n",
        "    TCDScheduler,\n",
        "    DiffusionPipeline,\n",
        ")\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "print(\"✓ Imports completed\")\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "device = \"cuda\"\n",
        "weights_dtype = torch.bfloat16\n",
        "basemodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "\n",
        "\n",
        "print(f\"✓ Configuration set - Device: {device}, Dtype: {weights_dtype}\")\n",
        "#print(f\"✓ Available models: {list(model_configs.keys())}\")\n",
        "\n",
        "\n",
        "\n",
        "def load_model(distillation_type=None, weights_dtype=torch.float16, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load SDXL models with specified distillation type.\n",
        "    \n",
        "    Returns:\n",
        "      'base'/'None': (pipe, base_unet, base_scheduler)\n",
        "      others:       (pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler)\n",
        "    \"\"\"\n",
        "    kind = ('base' if distillation_type in (None, 'base') else distillation_type).lower()\n",
        "    print(f\"Loading {kind.upper()} model...\")\n",
        "\n",
        "    # ---- base (always build this once for config/safety) ----\n",
        "    base_unet = UNet2DConditionModel.from_pretrained(\n",
        "        basemodel_id, subfolder=\"unet\", torch_dtype=weights_dtype\n",
        "    ).to(device)\n",
        "\n",
        "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "        basemodel_id,\n",
        "        unet=base_unet,\n",
        "        torch_dtype=weights_dtype,\n",
        "        use_safetensors=True,\n",
        "    )\n",
        "    base_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.scheduler = base_scheduler\n",
        "    pipe.to(device=device, dtype=weights_dtype)\n",
        "\n",
        "    if kind == 'base':\n",
        "        return pipe, base_unet, base_scheduler\n",
        "\n",
        "    # fresh UNet matching base config (required for state_dict load)\n",
        "    distilled_unet = UNet2DConditionModel.from_config(pipe.unet.config).to(device, dtype=weights_dtype)\n",
        "\n",
        "    if kind == 'dmd':\n",
        "        repo_name, ckpt_name = \"tianweiy/DMD2\", \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
        "        state = torch.load(hf_hub_download(repo_name, ckpt_name), map_location='cpu')\n",
        "        distilled_unet.load_state_dict(state if isinstance(state, dict) else state['state_dict'])\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'lightning':\n",
        "        repo, ckpt = \"ByteDance/SDXL-Lightning\", \"sdxl_lightning_4step_unet.safetensors\"\n",
        "        state = load_file(hf_hub_download(repo, ckpt))\n",
        "        distilled_unet.load_state_dict(state, strict=True)\n",
        "        # FIX: Use EulerDiscreteScheduler with trailing timesteps for both schedulers\n",
        "        distilled_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "        base_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif kind == 'turbo':\n",
        "        # turbo ships a full UNet; pull that directly\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
        "            \"stabilityai/sdxl-turbo\", subfolder=\"unet\", torch_dtype=weights_dtype, variant=\"fp16\"\n",
        "        ).to(device)\n",
        "        distilled_scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif kind == 'lcm':\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
        "            \"latent-consistency/lcm-sdxl\", torch_dtype=weights_dtype\n",
        "        ).to(device)\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'hyper':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"ByteDance/Hyper-SD\",\n",
        "                               weight_name=\"Hyper-SDXL-8steps-CFG-lora.safetensors\",\n",
        "                               adapter_name=\"hyper-sdxl-8step\")\n",
        "        pipe.set_adapters([\"hyper-sdxl-8step\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'pcm':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"wangfuyun/PCM_Weights\",\n",
        "                               weight_name=\"pcm_sdxl_smallcfg_4step_converted.safetensors\",\n",
        "                               subfolder=\"sdxl\",\n",
        "                               adapter_name=\"pcm-lora\")\n",
        "        pipe.set_adapters([\"pcm-lora\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif kind == 'tcd':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"h1t/TCD-SDXL-LoRA\", adapter_name=\"tcd-lora\")\n",
        "        pipe.set_adapters([\"tcd-lora\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'flash':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"jasperai/flash-sdxl\",\n",
        "                               weight_name=\"pytorch_lora_weights.safetensors\",\n",
        "                               adapter_name=\"flash-sdxl\")\n",
        "        pipe.set_adapters([\"flash-sdxl\"], adapter_weights=[1.0])\n",
        "        pipe.fuse_lora()\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown distillation type: '{distillation_type}'. \"\n",
        "                         f\"Available: {', '.join(sorted(model_configs.keys()))}\")\n",
        "\n",
        "    # IMPORTANT: actually use the distilled UNet\n",
        "    if hasattr(pipe, \"unet\") and distilled_unet is not pipe.unet:\n",
        "        pipe.unet = distilled_unet\n",
        "    pipe.scheduler = distilled_scheduler\n",
        "    pipe.to(device=device, dtype=weights_dtype)\n",
        "    return pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
        "\n",
        "\n",
        "def load_pipe(distillation_type='base'):\n",
        "    \"\"\"\n",
        "    Returns a ready-to-sample pipeline with the correct UNet and scheduler.\n",
        "    \"\"\"\n",
        "    pipe_result = load_model(distillation_type, weights_dtype, device)\n",
        "    # result already sets the right scheduler/UNet when not 'base'\n",
        "    pipe = pipe_result[0]\n",
        "    print(f\"✓ {('base' if distillation_type in (None, 'base') else distillation_type).upper()} pipeline ready\")\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b0adb1",
      "metadata": {},
      "source": [
        "## Test Across Select Models\n",
        "Run tests on multiple models with first 10 prompts and first seed, organized by model folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1774ad30",
      "metadata": {
        "gather": {
          "logged": 1761377940279
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10 prompts from /home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_noun_negative.json\n",
            "Output base directory: /home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results\n",
            "Loading FLASH model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe229652ea0545d09cf8499f5cba92d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50ab5afeff9945479a5dc4cd7d00c264",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OSError",
          "evalue": "Can't load the model for 'jasperai/flash-sdxl'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jasperai/flash-sdxl' is the correct path to a directory containing a file named pytorch_lora_weights.safetensors",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/diffusers/utils/hub_utils.py:289\u001b[39m, in \u001b[36m_get_model_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, weights_name, subfolder, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, dduf_entries)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# 2. Load model file as usual\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     model_file = \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_file\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/huggingface_hub/file_download.py:1125\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1124\u001b[39m os.makedirs(os.path.dirname(blob_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpointer_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[38;5;66;03m# if passed revision is not identical to commit_hash\u001b[39;00m\n\u001b[32m   1128\u001b[39m \u001b[38;5;66;03m# then revision has to be a branch name or tag name.\u001b[39;00m\n\u001b[32m   1129\u001b[39m \u001b[38;5;66;03m# In that case store a ref.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/os.py:218\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/os.py:228\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
            "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/home/azureuser/.cache/huggingface/hub/models--jasperai--flash-sdxl/snapshots'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m cfg = model_config[\u001b[33m\"\u001b[39m\u001b[33mrecommended_cfg\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Load the model pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m pipe = \u001b[43mload_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Create model-specific output directory\u001b[39;00m\n\u001b[32m     43\u001b[39m model_output_dir = os.path.join(output_base_dir, model_name)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mload_pipe\u001b[39m\u001b[34m(distillation_type)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_pipe\u001b[39m(distillation_type=\u001b[33m'\u001b[39m\u001b[33mbase\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    146\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[33;03m    Returns a ready-to-sample pipeline with the correct UNet and scheduler.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     pipe_result = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistillation_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# result already sets the right scheduler/UNet when not 'base'\u001b[39;00m\n\u001b[32m    151\u001b[39m     pipe = pipe_result[\u001b[32m0\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(distillation_type, weights_dtype, device)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m kind == \u001b[33m'\u001b[39m\u001b[33mflash\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    124\u001b[39m     pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_lora_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjasperai/flash-sdxl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpytorch_lora_weights.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                           \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash-sdxl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     pipe.set_adapters([\u001b[33m\"\u001b[39m\u001b[33mflash-sdxl\u001b[39m\u001b[33m\"\u001b[39m], adapter_weights=[\u001b[32m1.0\u001b[39m])\n\u001b[32m    129\u001b[39m     pipe.fuse_lora()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/diffusers/loaders/lora_pipeline.py:678\u001b[39m, in \u001b[36mStableDiffusionXLLoraLoaderMixin.load_lora_weights\u001b[39m\u001b[34m(self, pretrained_model_name_or_path_or_dict, adapter_name, hotswap, **kwargs)\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;66;03m# First, ensure that the checkpoint is a compatible one and can be successfully loaded.\u001b[39;00m\n\u001b[32m    677\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mreturn_lora_metadata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m state_dict, network_alphas, metadata = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlora_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43munet_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m is_correct_format = \u001b[38;5;28mall\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlora\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict.keys())\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_correct_format:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/diffusers/loaders/lora_pipeline.py:800\u001b[39m, in \u001b[36mStableDiffusionXLLoraLoaderMixin.lora_state_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path_or_dict, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m     allow_pickle = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    798\u001b[39m user_agent = {\u001b[33m\"\u001b[39m\u001b[33mfile_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mattn_procs_weights\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mframework\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m state_dict, metadata = \u001b[43m_fetch_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    814\u001b[39m is_dora_scale_present = \u001b[38;5;28many\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdora_scale\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m state_dict)\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_dora_scale_present:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/diffusers/loaders/lora_base.py:256\u001b[39m, in \u001b[36m_fetch_state_dict\u001b[39m\u001b[34m(pretrained_model_name_or_path_or_dict, weight_name, use_safetensors, local_files_only, cache_dir, force_download, proxies, token, revision, subfolder, user_agent, allow_pickle, metadata)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    253\u001b[39m     weight_name = _best_guess_weight_name(\n\u001b[32m    254\u001b[39m         pretrained_model_name_or_path_or_dict, file_extension=\u001b[33m\"\u001b[39m\u001b[33m.bin\u001b[39m\u001b[33m\"\u001b[39m, local_files_only=local_files_only\n\u001b[32m    255\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m model_file = \u001b[43m_get_model_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLORA_WEIGHT_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m state_dict = load_state_dict(model_file)\n\u001b[32m    269\u001b[39m metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/lib/python3.13/site-packages/diffusers/utils/hub_utils.py:332\u001b[39m, in \u001b[36m_get_model_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, weights_name, subfolder, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, dduf_entries)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    325\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWe couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt connect to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m to load this model, couldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find it\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    326\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in the cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not the path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    329\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m offline mode at \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/docs/diffusers/installation#offline-mode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    330\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    333\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load the model for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load it from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the same name. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    335\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOtherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a directory \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcontaining a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mOSError\u001b[39m: Can't load the model for 'jasperai/flash-sdxl'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jasperai/flash-sdxl' is the correct path to a directory containing a file named pytorch_lora_weights.safetensors"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------- Load prompts from JSON file ----------\n",
        "prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_noun_negative.json\"\n",
        "output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results\"\n",
        "\n",
        "model_configs = {\n",
        "    'base': {'steps': 100, 'recommended_cfg': 5.0},\n",
        "    'dmd': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'turbo': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "    'lcm': {'steps': 4, 'recommended_cfg': 1.0},\n",
        "    'hyper': {'steps': 8, 'recommended_cfg': 5.0},\n",
        "   'pcm': {'steps': 4, 'recommended_cfg': 2.0},\n",
        "}\n",
        "\n",
        "# Load prompts\n",
        "with open(prompts_file, 'r') as f:\n",
        "    prompts_data = json.load(f)\n",
        "\n",
        "# Use only first 10 prompts\n",
        "prompts_data = prompts_data[:10]\n",
        "\n",
        "print(f\"Loaded {len(prompts_data)} prompts from {prompts_file}\")\n",
        "print(f\"Output base directory: {output_base_dir}\")\n",
        "\n",
        "# ---------- Generate and save images organized by folder ----------\n",
        "total_generated = 0\n",
        "\n",
        "for model_name, model_config in model_configs.items():\n",
        "    steps = model_config[\"steps\"]\n",
        "    cfg = model_config[\"recommended_cfg\"]\n",
        "    \n",
        "    # Load the model pipeline\n",
        "    pipe = load_pipe(model_name)\n",
        "    \n",
        "    # Create model-specific output directory\n",
        "    model_output_dir = os.path.join(output_base_dir, model_name)\n",
        "    os.makedirs(model_output_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "    print(f\"Output directory: {model_output_dir}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    generated_count = 0\n",
        "    \n",
        "    for idx, item in enumerate(tqdm(prompts_data, desc=f\"{model_name} progress\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "        negative_prompt = item[\"negative_prompt\"]\n",
        "        seeds = item.get(\"seeds\", [42])\n",
        "        \n",
        "        # Use first seed for quick generation\n",
        "        seed = 2014\n",
        "        \n",
        "        try:\n",
        "            # Generate image\n",
        "            generator = torch.Generator(device=device).manual_seed(seed)\n",
        "            image = pipe(\n",
        "                prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                guidance_scale=cfg,\n",
        "                num_inference_steps=steps,\n",
        "                generator=generator\n",
        "            ).images[0]\n",
        "            \n",
        "            # Create output filename\n",
        "            group = item.get(\"group\", \"unknown\")\n",
        "            filename = f\"{idx:04d}_{group}_{seed}.png\"\n",
        "            filepath = os.path.join(model_output_dir, filename)\n",
        "            \n",
        "            # Save image\n",
        "            image.save(filepath)\n",
        "            generated_count += 1\n",
        "            total_generated += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image for prompt {idx} with {model_name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n✓ {model_name}: Generated and saved {generated_count} images\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"✓ Total generated and saved: {total_generated} images\")\n",
        "print(f\"✓ Models tested: {list(model_configs.keys())}\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "nag"
    },
    "kernelspec": {
      "display_name": "NAG (Python 3.13)",
      "language": "python",
      "name": "nag"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
