{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef94728",
   "metadata": {},
   "source": [
    "# SDXL Model Pipeline Setup - Lightning Fix Applied\n",
    "Supports 9 distillation models with proper scheduler configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02129d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "✓ Imports completed\n",
      "✓ Configuration set - Device: cuda, Dtype: torch.bfloat16\n",
      "✓ Available models: ['dmd', 'turbo', 'lightning', 'lcm', 'hyper', 'pcm', 'tcd', 'flash']\n"
     ]
    }
   ],
   "source": [
    "# Imports and Configuration\n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import (\n",
    "    UNet2DConditionModel,\n",
    "    StableDiffusionXLPipeline,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    EulerDiscreteScheduler,\n",
    "    DDIMScheduler,\n",
    "    LCMScheduler,\n",
    "    TCDScheduler,\n",
    "    DiffusionPipeline,\n",
    ")\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"✓ Imports completed\")\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "device = \"cuda\"\n",
    "weights_dtype = torch.bfloat16\n",
    "basemodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "model_configs = {\n",
    "#    'base': {'steps': 100, 'recommended_cfg': 5.0},\n",
    "    'dmd': {'steps': 4, 'recommended_cfg': 0.0},\n",
    "    'turbo': {'steps': 4, 'recommended_cfg': 0.0},\n",
    "    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
    "    'lcm': {'steps': 4, 'recommended_cfg': 1.0},\n",
    "    'hyper': {'steps': 8, 'recommended_cfg': 5.0},\n",
    "    'pcm': {'steps': 4, 'recommended_cfg': 2.0},\n",
    "    'tcd': {'steps': 4, 'recommended_cfg': 3.0},\n",
    "    'flash': {'steps': 4, 'recommended_cfg': 2.0}\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration set - Device: {device}, Dtype: {weights_dtype}\")\n",
    "print(f\"✓ Available models: {list(model_configs.keys())}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_model(distillation_type=None, weights_dtype=torch.float16, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load SDXL models with specified distillation type.\n",
    "    \n",
    "    Returns:\n",
    "      'base'/'None': (pipe, base_unet, base_scheduler)\n",
    "      others:       (pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler)\n",
    "    \"\"\"\n",
    "    kind = ('base' if distillation_type in (None, 'base') else distillation_type).lower()\n",
    "    print(f\"Loading {kind.upper()} model...\")\n",
    "\n",
    "    # ---- base (always build this once for config/safety) ----\n",
    "    base_unet = UNet2DConditionModel.from_pretrained(\n",
    "        basemodel_id, subfolder=\"unet\", torch_dtype=weights_dtype\n",
    "    ).to(device)\n",
    "\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        basemodel_id,\n",
    "        unet=base_unet,\n",
    "        torch_dtype=weights_dtype,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    base_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.scheduler = base_scheduler\n",
    "    pipe.to(device=device, dtype=weights_dtype)\n",
    "\n",
    "    if kind == 'base':\n",
    "        return pipe, base_unet, base_scheduler\n",
    "\n",
    "    # fresh UNet matching base config (required for state_dict load)\n",
    "    distilled_unet = UNet2DConditionModel.from_config(pipe.unet.config).to(device, dtype=weights_dtype)\n",
    "\n",
    "    if kind == 'dmd':\n",
    "        repo_name, ckpt_name = \"tianweiy/DMD2\", \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
    "        state = torch.load(hf_hub_download(repo_name, ckpt_name), map_location='cpu')\n",
    "        distilled_unet.load_state_dict(state if isinstance(state, dict) else state['state_dict'])\n",
    "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    elif kind == 'lightning':\n",
    "        repo, ckpt = \"ByteDance/SDXL-Lightning\", \"sdxl_lightning_4step_unet.safetensors\"\n",
    "        state = load_file(hf_hub_download(repo, ckpt))\n",
    "        distilled_unet.load_state_dict(state, strict=True)\n",
    "        # FIX: Use EulerDiscreteScheduler with trailing timesteps for both schedulers\n",
    "        distilled_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "        base_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "    elif kind == 'turbo':\n",
    "        # turbo ships a full UNet; pull that directly\n",
    "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
    "            \"stabilityai/sdxl-turbo\", subfolder=\"unet\", torch_dtype=weights_dtype, variant=\"fp16\"\n",
    "        ).to(device)\n",
    "        distilled_scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "    elif kind == 'lcm':\n",
    "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
    "            \"latent-consistency/lcm-sdxl\", torch_dtype=weights_dtype\n",
    "        ).to(device)\n",
    "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    elif kind == 'hyper':\n",
    "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
    "        pipe.load_lora_weights(\"ByteDance/Hyper-SD\",\n",
    "                               weight_name=\"Hyper-SDXL-8steps-CFG-lora.safetensors\",\n",
    "                               adapter_name=\"hyper-sdxl-8step\")\n",
    "        pipe.set_adapters([\"hyper-sdxl-8step\"], adapter_weights=[1.0])\n",
    "        distilled_unet = pipe.unet\n",
    "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    elif kind == 'pcm':\n",
    "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
    "        pipe.load_lora_weights(\"wangfuyun/PCM_Weights\",\n",
    "                               weight_name=\"pcm_sdxl_smallcfg_4step_converted.safetensors\",\n",
    "                               subfolder=\"sdxl\",\n",
    "                               adapter_name=\"pcm-lora\")\n",
    "        pipe.set_adapters([\"pcm-lora\"], adapter_weights=[1.0])\n",
    "        distilled_unet = pipe.unet\n",
    "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "    elif kind == 'tcd':\n",
    "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
    "        pipe.load_lora_weights(\"h1t/TCD-SDXL-LoRA\", adapter_name=\"tcd-lora\")\n",
    "        pipe.set_adapters([\"tcd-lora\"], adapter_weights=[1.0])\n",
    "        distilled_unet = pipe.unet\n",
    "        distilled_scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    elif kind == 'flash':\n",
    "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
    "        pipe.load_lora_weights(\"jasperai/flash-sdxl\",\n",
    "                               weight_name=\"pytorch_lora_weights.safetensors\",\n",
    "                               adapter_name=\"flash-sdxl\")\n",
    "        pipe.set_adapters([\"flash-sdxl\"], adapter_weights=[1.0])\n",
    "        pipe.fuse_lora()\n",
    "        distilled_unet = pipe.unet\n",
    "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distillation type: '{distillation_type}'. \"\n",
    "                         f\"Available: {', '.join(sorted(model_configs.keys()))}\")\n",
    "\n",
    "    # IMPORTANT: actually use the distilled UNet\n",
    "    if hasattr(pipe, \"unet\") and distilled_unet is not pipe.unet:\n",
    "        pipe.unet = distilled_unet\n",
    "    pipe.scheduler = distilled_scheduler\n",
    "    pipe.to(device=device, dtype=weights_dtype)\n",
    "    return pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
    "\n",
    "\n",
    "def load_pipe(distillation_type='base'):\n",
    "    \"\"\"\n",
    "    Returns a ready-to-sample pipeline with the correct UNet and scheduler.\n",
    "    \"\"\"\n",
    "    pipe_result = load_model(distillation_type, weights_dtype, device)\n",
    "    # result already sets the right scheduler/UNet when not 'base'\n",
    "    pipe = pipe_result[0]\n",
    "    print(f\"✓ {('base' if distillation_type in (None, 'base') else distillation_type).upper()} pipeline ready\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0adb1",
   "metadata": {},
   "source": [
    "## Test Across Select Models\n",
    "Run tests on multiple models with first 10 prompts and first seed, organized by model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774ad30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 prompts from /home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_noun_negative.json\n",
      "Output base directory: /home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results\n",
      "Loading DMD model...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Load prompts from JSON file ----------\n",
    "prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_noun_negative.json\"\n",
    "output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results\"\n",
    "\n",
    "# Load prompts\n",
    "with open(prompts_file, 'r') as f:\n",
    "    prompts_data = json.load(f)\n",
    "\n",
    "# Use only first 10 prompts\n",
    "prompts_data = prompts_data[:10]\n",
    "\n",
    "print(f\"Loaded {len(prompts_data)} prompts from {prompts_file}\")\n",
    "print(f\"Output base directory: {output_base_dir}\")\n",
    "\n",
    "# ---------- Generate and save images organized by folder ----------\n",
    "total_generated = 0\n",
    "\n",
    "for model_name, model_config in model_configs.items():\n",
    "    steps = model_config[\"steps\"]\n",
    "    cfg = model_config[\"recommended_cfg\"]\n",
    "    \n",
    "    # Load the model pipeline\n",
    "    pipe = load_pipe(model_name)\n",
    "    \n",
    "    # Create model-specific output directory\n",
    "    model_output_dir = os.path.join(output_base_dir, model_name)\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing model: {model_name}\")\n",
    "    print(f\"Output directory: {model_output_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    generated_count = 0\n",
    "    \n",
    "    for idx, item in enumerate(tqdm(prompts_data, desc=f\"{model_name} progress\")):\n",
    "        prompt = item[\"prompt\"]\n",
    "        negative_prompt = item[\"negative_prompt\"]\n",
    "        seeds = item.get(\"seeds\", [42])\n",
    "        \n",
    "        # Use first seed for quick generation\n",
    "        seed = 2014\n",
    "        \n",
    "        try:\n",
    "            # Generate image\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "            image = pipe(\n",
    "                prompt,\n",
    "                guidance_scale=cfg,\n",
    "                num_inference_steps=steps,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "            \n",
    "            # Create output filename\n",
    "            group = item.get(\"group\", \"unknown\")\n",
    "            filename = f\"{idx:04d}_{group}_{seed}.png\"\n",
    "            filepath = os.path.join(model_output_dir, filename)\n",
    "            \n",
    "            # Save image\n",
    "            image.save(filepath)\n",
    "            generated_count += 1\n",
    "            total_generated += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating image for prompt {idx} with {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ {model_name}: Generated and saved {generated_count} images\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Total generated and saved: {total_generated} images\")\n",
    "print(f\"✓ Models tested: {list(model_configs.keys())}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NAG Virtual Environment",
   "language": "python",
   "name": "nag-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
