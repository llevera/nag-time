{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ef94728",
      "metadata": {},
      "source": [
        "# SDXL Model Pipeline Setup - Lightning Fix Applied\n",
        "Supports 9 distillation models with proper scheduler configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02129d5a",
      "metadata": {
        "gather": {
          "logged": 1761414008640
        }
      },
      "outputs": [],
      "source": [
        "# Imports and Configuration\n",
        "import sys\n",
        "import torch\n",
        "from typing import Union, List, Optional, Dict, Any, Tuple, Callable, Type\n",
        "from PIL import Image\n",
        "from diffusers import (\n",
        "    UNet2DConditionModel,\n",
        "    StableDiffusionXLPipeline,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    LCMScheduler,\n",
        "    TCDScheduler,\n",
        "    DiffusionPipeline,\n",
        ")\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "print(\"✓ Imports completed\")\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "device = \"cuda\"\n",
        "weights_dtype = torch.bfloat16\n",
        "basemodel_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "\n",
        "\n",
        "print(f\"✓ Configuration set - Device: {device}, Dtype: {weights_dtype}\")\n",
        "#print(f\"✓ Available models: {list(model_configs.keys())}\")\n",
        "\n",
        "\n",
        "def load_model(pipeline_cls: Type[DiffusionPipeline], distillation_type=None, weights_dtype=torch.float16, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load SDXL models with specified distillation type.\n",
        "    \n",
        "    Returns:\n",
        "      'base'/'None': (pipe, base_unet, base_scheduler)\n",
        "      others:       (pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler)\n",
        "    \"\"\"\n",
        "    kind = ('base' if distillation_type in (None, 'base') else distillation_type).lower()\n",
        "    print(f\"Loading {kind.upper()} model...\")\n",
        "\n",
        "    # ---- base (always build this once for config/safety) ----\n",
        "    base_unet = UNet2DConditionModel.from_pretrained(\n",
        "        basemodel_id, subfolder=\"unet\", torch_dtype=weights_dtype\n",
        "    ).to(device)\n",
        "\n",
        "    pipe = pipeline_cls.from_pretrained(\n",
        "        basemodel_id,\n",
        "        unet=base_unet,\n",
        "        torch_dtype=weights_dtype,\n",
        "        use_safetensors=True,\n",
        "    )\n",
        "    base_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.scheduler = base_scheduler\n",
        "    pipe.to(device=device, dtype=weights_dtype)\n",
        "\n",
        "    if kind == 'base':\n",
        "        return pipe, base_unet, base_scheduler\n",
        "\n",
        "    # fresh UNet matching base config (required for state_dict load)\n",
        "    distilled_unet = UNet2DConditionModel.from_config(pipe.unet.config).to(device, dtype=weights_dtype)\n",
        "\n",
        "    if kind == 'dmd':\n",
        "        repo_name, ckpt_name = \"tianweiy/DMD2\", \"dmd2_sdxl_4step_unet_fp16.bin\"\n",
        "        state = torch.load(hf_hub_download(repo_name, ckpt_name), map_location='cpu')\n",
        "        distilled_unet.load_state_dict(state if isinstance(state, dict) else state['state_dict'])\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'lightning':\n",
        "        repo, ckpt = \"ByteDance/SDXL-Lightning\", \"sdxl_lightning_4step_unet.safetensors\"\n",
        "        state = load_file(hf_hub_download(repo, ckpt))\n",
        "        distilled_unet.load_state_dict(state, strict=True)\n",
        "        # FIX: Use EulerDiscreteScheduler with trailing timesteps for both schedulers\n",
        "        distilled_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "        base_scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif kind == 'turbo':\n",
        "        # turbo ships a full UNet; pull that directly\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
        "            \"stabilityai/sdxl-turbo\", subfolder=\"unet\", torch_dtype=weights_dtype, variant=\"fp16\"\n",
        "        ).to(device)\n",
        "        distilled_scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif kind == 'lcm':\n",
        "        distilled_unet = UNet2DConditionModel.from_pretrained(\n",
        "            \"latent-consistency/lcm-sdxl\", torch_dtype=weights_dtype\n",
        "        ).to(device)\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'hyper':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"ByteDance/Hyper-SD\",\n",
        "                               weight_name=\"Hyper-SDXL-8steps-CFG-lora.safetensors\",\n",
        "                               adapter_name=\"hyper-sdxl-8step\")\n",
        "        pipe.set_adapters([\"hyper-sdxl-8step\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'pcm':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"wangfuyun/PCM_Weights\",\n",
        "                               weight_name=\"pcm_sdxl_smallcfg_4step_converted.safetensors\",\n",
        "                               subfolder=\"sdxl\",\n",
        "                               adapter_name=\"pcm-lora\")\n",
        "        pipe.set_adapters([\"pcm-lora\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    elif kind == 'tcd':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"h1t/TCD-SDXL-LoRA\", adapter_name=\"tcd-lora\")\n",
        "        pipe.set_adapters([\"tcd-lora\"], adapter_weights=[1.0])\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    elif kind == 'flash':\n",
        "        pipe = DiffusionPipeline.from_pretrained(basemodel_id, torch_dtype=weights_dtype)\n",
        "        pipe.load_lora_weights(\"jasperai/flash-sdxl\",\n",
        "                               weight_name=\"pytorch_lora_weights.safetensors\",\n",
        "                               adapter_name=\"flash-sdxl\")\n",
        "        pipe.set_adapters([\"flash-sdxl\"], adapter_weights=[1.0])\n",
        "        pipe.fuse_lora()\n",
        "        distilled_unet = pipe.unet\n",
        "        distilled_scheduler = LCMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown distillation type: '{distillation_type}'. \"\n",
        "                         f\"Available: {', '.join(sorted(model_configs.keys()))}\")\n",
        "\n",
        "    # IMPORTANT: actually use the distilled UNet\n",
        "    if hasattr(pipe, \"unet\") and distilled_unet is not pipe.unet:\n",
        "        pipe.unet = distilled_unet\n",
        "    pipe.scheduler = distilled_scheduler\n",
        "    pipe.to(device=device, dtype=weights_dtype)\n",
        "    return pipe, base_unet, base_scheduler, distilled_unet, distilled_scheduler\n",
        "\n",
        "\n",
        "def load_pipe(pipeline_cls: Type[DiffusionPipeline], distillation_type='base'):\n",
        "    \"\"\"\n",
        "    Returns a ready-to-sample pipeline with the correct UNet and scheduler.\n",
        "    \"\"\"\n",
        "    pipe_result = load_model(pipeline_cls, distillation_type, weights_dtype, device)\n",
        "    # result already sets the right scheduler/UNet when not 'base'\n",
        "    pipe = pipe_result[0]\n",
        "    print(f\"✓ {('base' if distillation_type in (None, 'base') else distillation_type).upper()} pipeline ready\")\n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c58c56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NAG Attention Processor - from attention_nag.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from diffusers.utils import deprecate\n",
        "from diffusers.models.attention_processor import Attention\n",
        "\n",
        "class NAGAttnProcessor2_0:\n",
        "    \"\"\"\n",
        "    Normalized Attention Guidance (NAG) attention processor using PyTorch 2.0's\n",
        "    scaled_dot_product_attention for efficient computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nag_scale: float = 1.0, nag_tau: float = 2.5, nag_alpha: float = 0.5):\n",
        "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
        "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
        "        self.nag_scale = nag_scale\n",
        "        self.nag_tau = nag_tau\n",
        "        self.nag_alpha = nag_alpha\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        attn,\n",
        "        hidden_states: torch.Tensor,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        temb: Optional[torch.Tensor] = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        if len(args) > 0 or kwargs.get(\"scale\", None) is not None:\n",
        "            deprecation_message = \"The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\"\n",
        "            deprecate(\"scale\", \"1.0.0\", deprecation_message)\n",
        "\n",
        "        residual = hidden_states\n",
        "        if attn.spatial_norm is not None:\n",
        "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
        "\n",
        "        input_ndim = hidden_states.ndim\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            batch_size, channel, height, width = hidden_states.shape\n",
        "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
        "\n",
        "        batch_size, sequence_length, _ = (\n",
        "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
        "        )\n",
        "\n",
        "        apply_guidance = self.nag_scale > 1 and encoder_hidden_states is not None\n",
        "        if apply_guidance:\n",
        "            origin_batch_size = batch_size - len(hidden_states)\n",
        "            assert batch_size / origin_batch_size in [2, 3, 4]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
        "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
        "\n",
        "        if attn.group_norm is not None:\n",
        "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "        query = attn.to_q(hidden_states)\n",
        "\n",
        "        if encoder_hidden_states is None:\n",
        "            encoder_hidden_states = hidden_states\n",
        "        elif attn.norm_cross:\n",
        "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
        "\n",
        "        key = attn.to_k(encoder_hidden_states)\n",
        "        value = attn.to_v(encoder_hidden_states)\n",
        "\n",
        "        inner_dim = key.shape[-1]\n",
        "        head_dim = inner_dim // attn.heads\n",
        "\n",
        "        if apply_guidance:\n",
        "            if batch_size == 2 * origin_batch_size:\n",
        "                query = query.tile(2, 1, 1)\n",
        "            else:\n",
        "                query = torch.cat((query, query[origin_batch_size:2 * origin_batch_size]), dim=0)\n",
        "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
        "\n",
        "        if attn.norm_q is not None:\n",
        "            query = attn.norm_q(query)\n",
        "        if attn.norm_k is not None:\n",
        "            key = attn.norm_k(key)\n",
        "\n",
        "        hidden_states = F.scaled_dot_product_attention(\n",
        "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
        "        )\n",
        "\n",
        "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
        "        hidden_states = hidden_states.to(query.dtype)\n",
        "\n",
        "        if apply_guidance:\n",
        "            hidden_states_negative = hidden_states[-origin_batch_size:]\n",
        "            if batch_size == 2 * origin_batch_size:\n",
        "                hidden_states_positive = hidden_states[:origin_batch_size]\n",
        "            else:\n",
        "                hidden_states_positive = hidden_states[origin_batch_size:2 * origin_batch_size]\n",
        "            hidden_states_guidance = hidden_states_positive * self.nag_scale - hidden_states_negative * (self.nag_scale - 1)\n",
        "            norm_positive = torch.norm(hidden_states_positive, p=1, dim=-1, keepdim=True).expand(*hidden_states_positive.shape)\n",
        "            norm_guidance = torch.norm(hidden_states_guidance, p=1, dim=-1, keepdim=True).expand(*hidden_states_guidance.shape)\n",
        "\n",
        "            scale = norm_guidance / norm_positive\n",
        "            hidden_states_guidance = hidden_states_guidance * torch.minimum(scale, scale.new_ones(1) * self.nag_tau) / scale\n",
        "\n",
        "            hidden_states_guidance = hidden_states_guidance * self.nag_alpha + hidden_states_positive * (1 - self.nag_alpha)\n",
        "\n",
        "            if batch_size == 2 * origin_batch_size:\n",
        "                hidden_states = hidden_states_guidance\n",
        "            elif batch_size == 3 * origin_batch_size:\n",
        "                hidden_states = torch.cat((hidden_states[:origin_batch_size], hidden_states_guidance), dim=0)\n",
        "            elif batch_size == 4 * origin_batch_size:\n",
        "                hidden_states = torch.cat((hidden_states[:origin_batch_size], hidden_states_guidance, hidden_states[2 * origin_batch_size:3 * origin_batch_size]), dim=0)\n",
        "\n",
        "        hidden_states = attn.to_out[0](hidden_states)\n",
        "        hidden_states = attn.to_out[1](hidden_states)\n",
        "\n",
        "        if input_ndim == 4:\n",
        "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
        "\n",
        "        if attn.residual_connection:\n",
        "            hidden_states = hidden_states + residual\n",
        "\n",
        "        hidden_states = hidden_states / attn.rescale_output_factor\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "print(\"✓ NAGAttnProcessor2_0 loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c35480c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NAG Pipeline with Timing - from pipeline_sdxl_impactful_nag.py\n",
        "import math\n",
        "from diffusers.callbacks import MultiPipelineCallbacks, PipelineCallback\n",
        "from diffusers.image_processor import PipelineImageInput\n",
        "from diffusers.utils import deprecate, is_torch_xla_available\n",
        "from diffusers.pipelines.stable_diffusion_xl.pipeline_output import StableDiffusionXLPipelineOutput\n",
        "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import (\n",
        "    retrieve_timesteps,\n",
        "    rescale_noise_cfg,\n",
        ")\n",
        "from typing import Union, List, Optional, Dict, Any, Tuple, Callable\n",
        "\n",
        "if is_torch_xla_available():\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    XLA_AVAILABLE = True\n",
        "else:\n",
        "    XLA_AVAILABLE = False\n",
        "\n",
        "# Now the timing-aware variant\n",
        "class NAGTimeStableDiffusionXLPipeline(StableDiffusionXLPipeline):\n",
        "\n",
        "    \"\"\"Base NAG pipeline that extends StableDiffusionXLPipeline\"\"\"\n",
        "    \n",
        "    @property\n",
        "    def do_normalized_attention_guidance(self):\n",
        "        return self._nag_scale > 1\n",
        "\n",
        "    def _set_nag_attn_processor(self, nag_scale, nag_tau=2.5, nag_alpha=0.5):\n",
        "        if self.do_normalized_attention_guidance:\n",
        "            attn_procs = {}\n",
        "            for name, origin_attn_processor in self.unet.attn_processors.items():\n",
        "                if \"attn2\" in name:\n",
        "                    attn_procs[name] = NAGAttnProcessor2_0(nag_scale=nag_scale, nag_tau=nag_tau, nag_alpha=nag_alpha)\n",
        "                else:\n",
        "                    attn_procs[name] = origin_attn_processor\n",
        "            self.unet.set_attn_processor(attn_procs)\n",
        "\n",
        "    \"\"\"\n",
        "    NAG with timing from 'Impact of Negative Prompts':\n",
        "    - Delay enabling NAG until nag_start fraction of the trajectory\n",
        "    - Optional linear ramp of nag_scale for nag_ramp_steps after start\n",
        "    - Keep existing nag_end behaviour\n",
        "\n",
        "    Backwards-compatible cool-down (time-based):\n",
        "    - Optional post-NAG cool-down that temporarily reduces CFG by a fraction.\n",
        "    - Disabled by default (nag_cooldown=0.0) to preserve existing behaviour.\n",
        "    \"\"\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]] = None,\n",
        "        prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        height: Optional[int] = None,\n",
        "        width: Optional[int] = None,\n",
        "        num_inference_steps: int = 50,\n",
        "        timesteps: List[int] = None,\n",
        "        sigmas: List[float] = None,\n",
        "        denoising_end: Optional[float] = None,\n",
        "        guidance_scale: float = 5.0,\n",
        "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
        "        num_images_per_prompt: Optional[int] = 1,\n",
        "        eta: float = 0.0,\n",
        "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "        latents: Optional[torch.Tensor] = None,\n",
        "        prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        negative_pooled_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        ip_adapter_image: Optional[PipelineImageInput] = None,\n",
        "        ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        guidance_rescale: float = 0.0,\n",
        "        original_size: Optional[Tuple[int, int]] = None,\n",
        "        crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        target_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_original_size: Optional[Tuple[int, int]] = None,\n",
        "        negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
        "        negative_target_size: Optional[Tuple[int, int]] = None,\n",
        "        clip_skip: Optional[int] = None,\n",
        "        callback_on_step_end: Optional[\n",
        "            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
        "        ] = None,\n",
        "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
        "\n",
        "        # NAG controls\n",
        "        nag_scale: float = 1.0,\n",
        "        nag_tau: float = 2.5,\n",
        "        nag_alpha: float = 0.5,\n",
        "        nag_negative_prompt: str = None,\n",
        "        nag_negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
        "        nag_end: float = 1.0,\n",
        "\n",
        "        # Timing knobs from Impact of Negative Prompts\n",
        "        nag_start: float = 0.2,          # start NAG after 20% of the trajectory\n",
        "        nag_ramp_steps: int = 0,         # optional soft-start; 0 disables ramp\n",
        "\n",
        "        # NEW: time-based cool-down (fully backward compatible when 0.0)\n",
        "        nag_cooldown: float = 0.0,          # fraction after nag_end to apply cool-down (0.0 = disabled)\n",
        "        nag_cooldown_cfg_drop: float = 0.2, # relative CFG reduction during cool-down (e.g., 0.2 = -20%)\n",
        "\n",
        "        **kwargs,\n",
        "    ):\n",
        "        callback = kwargs.pop(\"callback\", None)\n",
        "        callback_steps = kwargs.pop(\"callback_steps\", None)\n",
        "\n",
        "        if callback is not None:\n",
        "            deprecate(\n",
        "                \"callback\",\n",
        "                \"1.0.0\",\n",
        "                \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
        "            )\n",
        "        if callback_steps is not None:\n",
        "            deprecate(\n",
        "                \"callback_steps\",\n",
        "                \"1.0.0\",\n",
        "                \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
        "            )\n",
        "\n",
        "        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
        "            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
        "\n",
        "        # 0. Defaults\n",
        "        height = height or self.default_sample_size * self.vae_scale_factor\n",
        "        width = width or self.default_sample_size * self.vae_scale_factor\n",
        "        original_size = original_size or (height, width)\n",
        "        target_size = target_size or (height, width)\n",
        "\n",
        "        # 1. Check inputs\n",
        "        self.check_inputs(\n",
        "            prompt,\n",
        "            prompt_2,\n",
        "            height,\n",
        "            width,\n",
        "            callback_steps,\n",
        "            negative_prompt,\n",
        "            negative_prompt_2,\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "            ip_adapter_image,\n",
        "            ip_adapter_image_embeds,\n",
        "            callback_on_step_end_tensor_inputs,\n",
        "        )\n",
        "\n",
        "        self._guidance_scale = guidance_scale\n",
        "        self._guidance_rescale = guidance_rescale\n",
        "        self._clip_skip = clip_skip\n",
        "        self._cross_attention_kwargs = cross_attention_kwargs\n",
        "        self._denoising_end = denoising_end\n",
        "        self._interrupt = False\n",
        "        self._nag_scale = nag_scale\n",
        "\n",
        "        # 2. Batch size\n",
        "        if prompt is not None and isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif prompt is not None and isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "        device = self._execution_device\n",
        "\n",
        "        # 3. Encode prompts\n",
        "        lora_scale = self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
        "\n",
        "        (\n",
        "            prompt_embeds,\n",
        "            negative_prompt_embeds,\n",
        "            pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds,\n",
        "        ) = self.encode_prompt(\n",
        "            prompt=prompt,\n",
        "            prompt_2=prompt_2,\n",
        "            device=device,\n",
        "            num_images_per_prompt=num_images_per_prompt,\n",
        "            do_classifier_free_guidance=self.do_classifier_free_guidance or self.do_normalized_attention_guidance,\n",
        "            negative_prompt=negative_prompt,\n",
        "            negative_prompt_2=negative_prompt_2,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
        "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
        "            lora_scale=lora_scale,\n",
        "            clip_skip=self.clip_skip,\n",
        "        )\n",
        "\n",
        "        # Prepare NAG negative embeddings (but DO NOT append yet; we enable later)\n",
        "        if self.do_normalized_attention_guidance:\n",
        "            if nag_negative_prompt_embeds is None:\n",
        "                if nag_negative_prompt is None:\n",
        "                    if negative_prompt is not None:\n",
        "                        if self.do_classifier_free_guidance:\n",
        "                            nag_negative_prompt_embeds = negative_prompt_embeds\n",
        "                        else:\n",
        "                            nag_negative_prompt = negative_prompt\n",
        "                    else:\n",
        "                        nag_negative_prompt = \"\"\n",
        "                if nag_negative_prompt is not None and nag_negative_prompt_embeds is None:\n",
        "                    nag_negative_prompt_embeds = self.encode_prompt(\n",
        "                        prompt=nag_negative_prompt,\n",
        "                        device=device,\n",
        "                        num_images_per_prompt=num_images_per_prompt,\n",
        "                        do_classifier_free_guidance=False,\n",
        "                        lora_scale=lora_scale,\n",
        "                        clip_skip=self.clip_skip,\n",
        "                    )[0]\n",
        "\n",
        "        # 4. Timesteps\n",
        "        timesteps, num_inference_steps = retrieve_timesteps(\n",
        "            self.scheduler, num_inference_steps, device, timesteps, sigmas\n",
        "        )\n",
        "\n",
        "        # 5. Latents\n",
        "        num_channels_latents = self.unet.config.in_channels\n",
        "        latents = self.prepare_latents(\n",
        "            batch_size * num_images_per_prompt,\n",
        "            num_channels_latents,\n",
        "            height,\n",
        "            width,\n",
        "            prompt_embeds.dtype,\n",
        "            device,\n",
        "            generator,\n",
        "            latents,\n",
        "        )\n",
        "\n",
        "        # 6. Extra step kwargs\n",
        "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "        # 7. Added time ids & embeddings\n",
        "        add_text_embeds = pooled_prompt_embeds\n",
        "        if self.text_encoder_2 is None:\n",
        "            text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
        "        else:\n",
        "            text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
        "\n",
        "        add_time_ids = self._get_add_time_ids(\n",
        "            original_size,\n",
        "            crops_coords_top_left,\n",
        "            target_size,\n",
        "            dtype=prompt_embeds.dtype,\n",
        "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
        "        )\n",
        "        if negative_original_size is not None and negative_target_size is not None:\n",
        "            negative_add_time_ids = self._get_add_time_ids(\n",
        "                negative_original_size,\n",
        "                negative_crops_coords_top_left,\n",
        "                negative_target_size,\n",
        "                dtype=prompt_embeds.dtype,\n",
        "                text_encoder_projection_dim=text_encoder_projection_dim,\n",
        "            )\n",
        "        else:\n",
        "            negative_add_time_ids = add_time_ids\n",
        "\n",
        "        if self.do_classifier_free_guidance:\n",
        "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
        "            add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
        "            add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
        "\n",
        "        # NOTE: Delayed NAG — do NOT append nag_negative_prompt_embeds yet.\n",
        "\n",
        "        prompt_embeds = prompt_embeds.to(device)\n",
        "        add_text_embeds = add_text_embeds.to(device)\n",
        "        add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
        "\n",
        "        if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
        "            image_embeds = self.prepare_ip_adapter_image_embeds(\n",
        "                ip_adapter_image,\n",
        "                ip_adapter_image_embeds,\n",
        "                device,\n",
        "                batch_size * num_images_per_prompt,\n",
        "                self.do_classifier_free_guidance,\n",
        "            )\n",
        "\n",
        "        # 8. Denoising setup\n",
        "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
        "\n",
        "        # 8.1 denoising_end\n",
        "        if (\n",
        "            self.denoising_end is not None\n",
        "            and isinstance(self.denoising_end, float)\n",
        "            and 0 < self.denoising_end < 1\n",
        "        ):\n",
        "            discrete_timestep_cutoff = int(\n",
        "                round(\n",
        "                    self.scheduler.config.num_train_timesteps\n",
        "                    - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
        "                )\n",
        "            )\n",
        "            num_inference_steps = len([ts for ts in timesteps if ts >= discrete_timestep_cutoff])\n",
        "            timesteps = timesteps[:num_inference_steps]\n",
        "\n",
        "        # 9. Guidance scale embedding (base / default)\n",
        "        # For full backward-compatibility, we precompute the \"base\" embedding as before.\n",
        "        base_timestep_cond = None\n",
        "        if self.unet.config.time_cond_proj_dim is not None:\n",
        "            base_gs_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
        "            base_timestep_cond = self.get_guidance_scale_embedding(\n",
        "                base_gs_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
        "            ).to(device=device, dtype=latents.dtype)\n",
        "\n",
        "        # ---- NEW: timing state for NAG & cool-down ----\n",
        "        def _to_ddpm(frac: float) -> int:\n",
        "            # clamp to [0,1], map to DDPM 0..999, note timesteps are descending\n",
        "            frac = max(0.0, min(1.0, float(frac)))\n",
        "            return math.floor((1 - frac) * 999)\n",
        "\n",
        "        nag_start_t = _to_ddpm(nag_start)\n",
        "        nag_end_t = _to_ddpm(nag_end)\n",
        "\n",
        "        # Cool-down window [nag_end, nag_end + nag_cooldown] in fractional time, then map to DDPM t.\n",
        "        cooldown_active = float(nag_cooldown) > 0.0\n",
        "        if cooldown_active:\n",
        "            cooldown_end_frac = min(1.0, float(nag_end) + float(nag_cooldown))\n",
        "            cooldown_end_t = _to_ddpm(cooldown_end_frac)\n",
        "        else:\n",
        "            cooldown_end_t = None  # unused\n",
        "\n",
        "        origin_attn_procs = self.unet.attn_processors\n",
        "        attn_procs_applied = False\n",
        "        attn_procs_recovered = False\n",
        "        i_start = None  # loop index when NAG is enabled\n",
        "\n",
        "        self._num_timesteps = len(timesteps)\n",
        "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                if self.interrupt:\n",
        "                    continue\n",
        "\n",
        "                # Build latent input\n",
        "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
        "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "                # ---- Enable NAG once we reach nag_start ----\n",
        "                if (\n",
        "                    self.do_normalized_attention_guidance\n",
        "                    and not attn_procs_applied\n",
        "                    and t <= nag_start_t\n",
        "                ):\n",
        "                    # initial scale for ramp\n",
        "                    current_scale = 1.0 if nag_ramp_steps > 0 else nag_scale\n",
        "                    self._set_nag_attn_processor(current_scale, nag_tau, nag_alpha)\n",
        "\n",
        "                    # append the NAG negative branch now\n",
        "                    if nag_negative_prompt_embeds is not None:\n",
        "                        prompt_embeds = torch.cat([prompt_embeds, nag_negative_prompt_embeds], dim=0)\n",
        "\n",
        "                    attn_procs_applied = True\n",
        "                    i_start = i  # remember start index\n",
        "\n",
        "                # ---- Optionally ramp the nag_scale for a few steps after start ----\n",
        "                if (\n",
        "                    self.do_normalized_attention_guidance\n",
        "                    and attn_procs_applied\n",
        "                    and not attn_procs_recovered\n",
        "                    and nag_ramp_steps > 0\n",
        "                    and i_start is not None\n",
        "                ):\n",
        "                    steps_since_on = max(0, i - i_start)\n",
        "                    if steps_since_on <= nag_ramp_steps:\n",
        "                        ramped = 1.0 + (nag_scale - 1.0) * (steps_since_on / max(1, nag_ramp_steps))\n",
        "                        self._set_nag_attn_processor(ramped, nag_tau, nag_alpha)\n",
        "                    else:\n",
        "                        # ensure final scale is set once ramp completes\n",
        "                        if abs(self._nag_scale - nag_scale) > 1e-6:\n",
        "                            self._set_nag_attn_processor(nag_scale, nag_tau, nag_alpha)\n",
        "\n",
        "                # ---- Disable NAG when we pass nag_end ----\n",
        "                if (\n",
        "                    self.do_normalized_attention_guidance\n",
        "                    and attn_procs_applied\n",
        "                    and not attn_procs_recovered\n",
        "                    and t < nag_end_t\n",
        "                ):\n",
        "                    self.unet.set_attn_processor(origin_attn_procs)\n",
        "                    # drop the appended branch so shapes match again\n",
        "                    prompt_embeds = prompt_embeds[: len(latent_model_input)]\n",
        "                    attn_procs_recovered = True\n",
        "\n",
        "                # ---- Cool-down (time-based) effective CFG ----\n",
        "                if cooldown_active:\n",
        "                    # In DDPM indexing, smaller t means later in the trajectory.\n",
        "                    # We are \"in cool-down\" if we've gone *past* nag_end (t <= nag_end_t)\n",
        "                    # but not yet beyond the cool-down window end (t >= cooldown_end_t).\n",
        "                    in_cooldown = (t <= nag_end_t) and (t >= cooldown_end_t)\n",
        "                else:\n",
        "                    in_cooldown = False\n",
        "\n",
        "                if in_cooldown:\n",
        "                    current_guidance_scale = max(1.0, self.guidance_scale * (1.0 - float(nag_cooldown_cfg_drop)))\n",
        "                else:\n",
        "                    current_guidance_scale = self.guidance_scale\n",
        "\n",
        "                # Build timestep_cond:\n",
        "                # - If no cool-down (default), reuse base embedding (backwards-compatible).\n",
        "                # - If cool-down is active, rebuild embedding with the step's effective CFG.\n",
        "                if self.unet.config.time_cond_proj_dim is not None:\n",
        "                    if cooldown_active:\n",
        "                        gs_tensor = torch.tensor(current_guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
        "                        timestep_cond = self.get_guidance_scale_embedding(\n",
        "                            gs_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
        "                        ).to(device=device, dtype=latents.dtype)\n",
        "                    else:\n",
        "                        timestep_cond = base_timestep_cond\n",
        "                else:\n",
        "                    timestep_cond = None\n",
        "\n",
        "                # predict noise\n",
        "                added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
        "                if ip_adapter_image is not None or ip_adapter_image_embeds is not None:\n",
        "                    added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
        "\n",
        "                noise_pred = self.unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    timestep_cond=timestep_cond,\n",
        "                    cross_attention_kwargs=self.cross_attention_kwargs,\n",
        "                    added_cond_kwargs=added_cond_kwargs,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "\n",
        "                # CFG (use effective guidance for this step; identical to old behaviour when cool-down is off)\n",
        "                if self.do_classifier_free_guidance:\n",
        "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + current_guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "                if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
        "                    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
        "\n",
        "                # step scheduler\n",
        "                latents_dtype = latents.dtype\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
        "                if latents.dtype != latents_dtype and torch.backends.mps.is_available():\n",
        "                    latents = latents.to(latents_dtype)\n",
        "\n",
        "                # callbacks\n",
        "                if callback_on_step_end is not None:\n",
        "                    callback_kwargs = {}\n",
        "                    for k in callback_on_step_end_tensor_inputs:\n",
        "                        callback_kwargs[k] = locals()[k]\n",
        "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
        "\n",
        "                    latents = callback_outputs.pop(\"latents\", latents)\n",
        "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
        "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
        "                    add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
        "                    negative_pooled_prompt_embeds = callback_outputs.pop(\n",
        "                        \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n",
        "                    )\n",
        "                    add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
        "\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "                    if callback is not None and i % callback_steps == 0:\n",
        "                        step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
        "                        callback(step_idx, t, latents)\n",
        "\n",
        "                if XLA_AVAILABLE:\n",
        "                    xm.mark_step()\n",
        "\n",
        "        # decode\n",
        "        if output_type != \"latent\":\n",
        "            needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
        "            if needs_upcasting:\n",
        "                self.upcast_vae()\n",
        "                latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
        "            elif latents.dtype != self.vae.dtype and torch.backends.mps.is_available():\n",
        "                self.vae = self.vae.to(latents.dtype)\n",
        "\n",
        "            has_latents_mean = hasattr(self.vae.config, \"latents_mean\") and self.vae.config.latents_mean is not None\n",
        "            has_latents_std = hasattr(self.vae.config, \"latents_std\") and self.vae.config.latents_std is not None\n",
        "            if has_latents_mean and has_latents_std:\n",
        "                latents_mean = torch.tensor(self.vae.config.latents_mean).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
        "                latents_std = torch.tensor(self.vae.config.latents_std).view(1, 4, 1, 1).to(latents.device, latents.dtype)\n",
        "                latents = latents * latents_std / self.vae.config.scaling_factor + latents_mean\n",
        "            else:\n",
        "                latents = latents / self.vae.config.scaling_factor\n",
        "\n",
        "            image = self.vae.decode(latents, return_dict=False)[0]\n",
        "\n",
        "            if needs_upcasting:\n",
        "                self.vae.to(dtype=torch.float16)\n",
        "        else:\n",
        "            image = latents\n",
        "\n",
        "        if output_type != \"latent\":\n",
        "            if self.watermark is not None:\n",
        "                image = self.watermark.apply_watermark(image)\n",
        "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
        "\n",
        "        # ensure processors are restored\n",
        "        if self.do_normalized_attention_guidance and not attn_procs_recovered:\n",
        "            self.unet.set_attn_processor(origin_attn_procs)\n",
        "\n",
        "        self.maybe_free_model_hooks()\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image,)\n",
        "        return StableDiffusionXLPipelineOutput(images=image)\n",
        "\n",
        "print(\"✓ NAGTimeStableDiffusionXLPipeline loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b0adb1",
      "metadata": {},
      "source": [
        "## Test Across Select Models\n",
        "Run tests on multiple models with first 10 prompts and first seed, organized by model folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1774ad30",
      "metadata": {
        "gather": {
          "logged": 1761418568009
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display as notebook_display\n",
        "\n",
        "\n",
        "# ---------- Toggle dry-run mode ----------\n",
        "dry_run = False  # When True: process only 1 prompt and display images instead of saving\n",
        "\n",
        "# ---------- Load prompts from JSON file ----------\n",
        "prompts_file = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/data/prompts_noun_negative.json\"\n",
        "\n",
        "model_configs = {\n",
        "#    'base': {'steps': 30, 'recommended_cfg': 7.0},\n",
        "#    'dmd': {'steps': 4, 'recommended_cfg': 1.5},\n",
        "    'turbo': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "#    'lightning': {'steps': 4, 'recommended_cfg': 0.0},\n",
        "#    'lcm': {'steps': 4, 'recommended_cfg': 1.5},\n",
        "#    'hyper': {'steps': 8, 'recommended_cfg': 5.0},\n",
        "#    'pcm': {'steps': 4, 'recommended_cfg': 3.0},\n",
        "}\n",
        "\n",
        "use_nag = True  # Set to True to use NAG pipeline, False for standard SDXL pipeline\n",
        "\n",
        "# ---- Fixed seeds for reproducibility (3 seeds) ----\n",
        "fixed_seeds = [2025, 42, 1337]\n",
        "\n",
        "# -------- Helpers --------\n",
        "def write_or_display(image, filepath, title=\"preview\"):\n",
        "    if dry_run:\n",
        "        # Always display inline in the notebook\n",
        "        notebook_display(image)\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "        image.save(filepath)\n",
        "\n",
        "\n",
        "# Load prompts\n",
        "with open(prompts_file, 'r') as f:\n",
        "    prompts_data = json.load(f)\n",
        "\n",
        "# ---- Select top 50 prompts (or 1 in dry-run) ----\n",
        "if isinstance(prompts_data, list) and len(prompts_data) > 0 and isinstance(prompts_data[0], dict) and 'score' in prompts_data[0]:\n",
        "    prompts_data = sorted(prompts_data, key=lambda x: x.get('score', 0), reverse=True)[:50]\n",
        "else:\n",
        "    prompts_data = prompts_data[:50]\n",
        "\n",
        "if dry_run:\n",
        "    prompts_data = prompts_data[:1]  # only one prompt\n",
        "\n",
        "# ---- Choose seeds (all vs one when dry-run) ----\n",
        "seeds_to_use = fixed_seeds[:1] if dry_run else fixed_seeds\n",
        "\n",
        "print(f\"Loaded {len(prompts_data)} prompt(s) from {prompts_file}\")\n",
        "print(f\"[CONFIG] use_nag={use_nag} | models={list(model_configs.keys())} | dry_run={dry_run} | seeds={seeds_to_use}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Generate ----------\n",
        "total_generated = 0\n",
        "\n",
        "for model_name, model_config in model_configs.items():\n",
        "    steps = model_config[\"steps\"]\n",
        "    cfg = model_config[\"recommended_cfg\"]\n",
        "\n",
        "    if use_nag:\n",
        "        pipeline_cls = NAGTimeStableDiffusionXLPipeline\n",
        "        output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results-nag\"\n",
        "    else:\n",
        "        pipeline_cls = StableDiffusionXLPipeline\n",
        "        output_base_dir = \"/home/azureuser/cloudfiles/code/Users/Normalized-Attention-Guidance/results\"\n",
        "\n",
        "    print(f\"[MODEL] {model_name} -> steps={steps}, cfg={cfg}\")\n",
        "    print(f\"[PIPELINE] {pipeline_cls.__name__}\")\n",
        "    pipe = load_pipe(pipeline_cls, model_name)\n",
        "\n",
        "    model_output_dir = os.path.join(output_base_dir, model_name)\n",
        "\n",
        "    if use_nag:\n",
        "        baseline_dir  = os.path.join(model_output_dir, \"baseline\")\n",
        "        withneg0_dir  = os.path.join(model_output_dir, \"with_negative_start0\")\n",
        "        withneg02_dir = os.path.join(model_output_dir, \"with_negative_start0p2\")\n",
        "        if not dry_run:\n",
        "            os.makedirs(baseline_dir, exist_ok=True)\n",
        "            os.makedirs(withneg0_dir, exist_ok=True)\n",
        "            os.makedirs(withneg02_dir, exist_ok=True)\n",
        "        print(\"\\n[DIRS]\")\n",
        "        print(f\"  base_output: {output_base_dir}\")\n",
        "        print(f\"  model_root : {model_output_dir}\")\n",
        "        print(f\"  baseline   : {baseline_dir}\")\n",
        "        print(f\"  neg@0.0    : {withneg0_dir}\")\n",
        "        print(f\"  neg@0.2    : {withneg02_dir}\")\n",
        "    else:\n",
        "        baseline_dir = os.path.join(model_output_dir, \"baseline\")\n",
        "        withneg_dir  = os.path.join(model_output_dir, \"with_negative\")\n",
        "        if not dry_run:\n",
        "            os.makedirs(baseline_dir, exist_ok=True)\n",
        "            os.makedirs(withneg_dir, exist_ok=True)\n",
        "        print(\"\\n[DIRS]\")\n",
        "        print(f\"  base_output: {output_base_dir}\")\n",
        "        print(f\"  model_root : {model_output_dir}\")\n",
        "        print(f\"  baseline   : {baseline_dir}\")\n",
        "        print(f\"  with_neg   : {withneg_dir}\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing model: {model_name}\")\n",
        "    print(\"Displaying images (no writes)\\n\" if dry_run else f\"Writing under: {model_output_dir}\\n\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    generated_count = 0\n",
        "\n",
        "    for idx, item in enumerate(tqdm(prompts_data, desc=f\"{model_name} progress\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "        negative_prompt = item.get(\"negative_prompt\", \"\")\n",
        "\n",
        "        if use_nag:\n",
        "            runs = [\n",
        "                {\"out_dir\": baseline_dir,  \"neg\": False, \"nag_start\": None, \"neg_label\": \"noneg\"},\n",
        "                {\"out_dir\": withneg0_dir,  \"neg\": True,  \"nag_start\": 0.0,  \"neg_label\": \"neg_s0\"},\n",
        "                {\"out_dir\": withneg02_dir, \"neg\": True,  \"nag_start\": 0.2,  \"neg_label\": \"neg_s0p2\"},\n",
        "            ]\n",
        "\n",
        "            for run in runs:\n",
        "                for seed in seeds_to_use:\n",
        "                    try:\n",
        "                        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "                        call_kwargs = dict(\n",
        "                            prompt=prompt,\n",
        "                            guidance_scale=cfg,\n",
        "                            nag_scale = 3.0,\n",
        "                            num_inference_steps=steps,\n",
        "                            generator=generator\n",
        "                        )\n",
        "                        call_kwargs[\"nag_negative_prompt\"] = negative_prompt if run[\"neg\"] else None\n",
        "                        if run[\"nag_start\"] is not None:\n",
        "                            call_kwargs[\"nag_start\"] = run[\"nag_start\"]\n",
        "\n",
        "                        print(call_kwargs)\n",
        "\n",
        "                        image = pipe(**call_kwargs).images[0]\n",
        "                        filename = f\"{idx:04d}_{seed}_{run['neg_label']}.png\"\n",
        "                        filepath = os.path.join(run[\"out_dir\"], filename)\n",
        "\n",
        "                        if dry_run:\n",
        "                            print(f\"[DISPLAY] {model_name} ({run['neg_label']} | seed={seed})\")\n",
        "                            write_or_display(image, filepath, title=f\"{model_name}:{run['neg_label']}:{seed}\")\n",
        "                        else:\n",
        "                            print(f\"[WRITE] {model_name} -> {filepath}\")\n",
        "                            write_or_display(image, filepath)\n",
        "\n",
        "                        generated_count += 1\n",
        "                        total_generated += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] model={model_name}, prompt_idx={idx}, seed={seed}, run={run['neg_label']} -> {e}\")\n",
        "                        continue\n",
        "        else:\n",
        "            for use_negative in [False, True]:\n",
        "                out_dir = withneg_dir if use_negative else baseline_dir\n",
        "                neg_label = \"neg\" if use_negative else \"noneg\"\n",
        "\n",
        "                for seed in seeds_to_use:\n",
        "                    try:\n",
        "                        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "                        image = pipe(\n",
        "                            prompt,\n",
        "                            negative_prompt=negative_prompt if use_negative else None,\n",
        "                            guidance_scale=cfg,\n",
        "                            num_inference_steps=steps,\n",
        "                            generator=generator\n",
        "                        ).images[0]\n",
        "\n",
        "                        filename = f\"{idx:04d}_{seed}_{neg_label}.png\"\n",
        "                        filepath = os.path.join(out_dir, filename)\n",
        "\n",
        "                        if dry_run:\n",
        "                            print(f\"[DISPLAY] {model_name} ({neg_label} | seed={seed})\")\n",
        "                            write_or_display(image, filepath, title=f\"{model_name}:{neg_label}:{seed}\")\n",
        "                        else:\n",
        "                            print(f\"[WRITE] {model_name} -> {filepath}\")\n",
        "                            write_or_display(image, filepath)\n",
        "\n",
        "                        generated_count += 1\n",
        "                        total_generated += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"[ERROR] model={model_name}, prompt_idx={idx}, seed={seed}, use_negative={use_negative} -> {e}\")\n",
        "                        continue\n",
        "\n",
        "    print(f\"\\n✓ {model_name}: Generated and {'displayed' if dry_run else 'saved'} {generated_count} image(s)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"✓ Total generated and {'displayed' if dry_run else 'saved'}: {total_generated} image(s)\")\n",
        "print(f\"✓ Models tested: {list(model_configs.keys())}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "nag"
    },
    "kernelspec": {
      "display_name": "NAG (Python 3.13)",
      "language": "python",
      "name": "nag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
